![[streaming-flow.png]]
Spark streaming enables sclable, high-throughput, fault-tolerant stream processing of live data streams. Spark streaming provides a high level abstraction calles discretized stream or DStream, which represents a continious stream of data.

##### Discretized Streams [DStreams](https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams)
**Discretized Stream** or **DStream** is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset
![[streaming-dstream.png]]
![[streaming-dstream-ops.png]]


##### [Input DStreams and Receivers](https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers)
Input DStreams are DStreams representing the stream of input data received from streaming source.. Every input DStream (except file stream, discussed later in this section) is associated with a **Receiver** ([Scala doc](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/streaming/receiver/Receiver.html), [Java doc](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/receiver/Receiver.html)) object which receives the data from a source and stores it in Spark’s memory for processing.

Spark Streaming provides two categories of built-in streaming sources.

-   _Basic sources_: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.
-   _Advanced sources_: Sources like Kafka, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the [linking](https://spark.apache.org/docs/latest/streaming-programming-guide.html#linking) section.


###### Receiver Reliability
There can be two kinds of data sources based on their _reliability_. Sources (like Kafka) allow the transferred data to be acknowledged.
1.  _Reliable Receiver_ - A _reliable receiver_ correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication.
2.  _Unreliable Receiver_ - An _unreliable receiver_ does _not_ send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.



###### Transformation
- **map**(_func_) / **flatMap**(_func_)
- **filter**(_func_) **join**(_otherStream_, [_numTasks_])
- **count**() / **countByValue**()
- **reduce**(_func_) / **reduceByKey**(_func_, [_numTasks_]) / **transform**(_func_) / **updateStateByKey**(_func_)
- **cogroup**(_otherStream_, [_numTasks_])
- **repartition**(_numPartitions_) / **union**(_otherStream_)

- To look up 
	- Config properties
		- While creating spark conf : number of core/thread
	- Examine Kafka Reciever for Input DStream




##### [Window Operations](https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations)
Spark Streaming also provides _windowed computations_, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window. Every time the window _slides_ over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream.This shows that any window operation needs to specify two parameters.

-   _window length_ - The duration of the window (3 in the figure).
-   _sliding interval_ - The interval at which the window operation is performed (2 in the figure).

These two parameters must be multiples of the batch interval of the source DStream (1 in the figure).
![[streaming-dstream-window.png]]

#### Join Operations
You can perform different kinds of joins in Spark Streaming.(`leftOuterJoin`, `rightOuterJoin`, `fullOuterJoin`).

#### [Caching / Persistence](https://spark.apache.org/docs/latest/streaming-programming-guide.html#caching--persistence)
This is useful if the data in the DStream will be computed multiple times (e.g., multiple operations on the same data). For window-based operations like `reduceByWindow` and `reduceByKeyAndWindow` and state-based operations like `updateStateByKey`, this is implicitly true.

#### [Checkpointing](https://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing)
Spark Streaming needs to _checkpoint_ enough information to a fault- tolerant storage system such that it can recover from failures. There are two types of data that are checkpointed.

-   _Metadata checkpointing_ - Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later). Metadata includes:
    -   _Configuration_ - The configuration that was used to create the streaming application.
    -   _DStream operations_ - The set of DStream operations that define the streaming application.
    -   _Incomplete batches_ - Batches whose jobs are queued but have not completed yet.
-   _Data checkpointing_ - Saving of the generated RDDs to reliable storage. This is necessary in some _stateful_ transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically _checkpointed_ to reliable storage (e.g. HDFS) to cut off the dependency chains.

