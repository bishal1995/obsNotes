![[streaming-flow.png]]
Spark streaming enables sclable, high-throughput, fault-tolerant stream processing of live data streams. Spark streaming provides a high level abstraction calles discretized stream or DStream, which represents a continious stream of data.

##### Discretized Streams [DStreams](https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams)
**Discretized Stream** or **DStream** is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset
![[streaming-dstream.png]]
![[streaming-dstream-ops.png]]


##### [Input DStreams and Receivers](https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers)
Input DStreams are DStreams representing the stream of input data received from streaming source.. Every input DStream (except file stream, discussed later in this section) is associated with a **Receiver** ([Scala doc](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/streaming/receiver/Receiver.html), [Java doc](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/receiver/Receiver.html)) object which receives the data from a source and stores it in Spark’s memory for processing.

Spark Streaming provides two categories of built-in streaming sources.

-   _Basic sources_: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.
-   _Advanced sources_: Sources like Kafka, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the [linking](https://spark.apache.org/docs/latest/streaming-programming-guide.html#linking) section.


###### Receiver Reliability
There can be two kinds of data sources based on their _reliability_. Sources (like Kafka) allow the transferred data to be acknowledged.
1.  _Reliable Receiver_ - A _reliable receiver_ correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication.
2.  _Unreliable Receiver_ - An _unreliable receiver_ does _not_ send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.



###### Transformation
- **map**(_func_) / **flatMap**(_func_)
- **filter**(_func_) **join**(_otherStream_, [_numTasks_])
- **count**() / **countByValue**()
- **reduce**(_func_) / **reduceByKey**(_func_, [_numTasks_]) / **transform**(_func_) / **updateStateByKey**(_func_)
- **cogroup**(_otherStream_, [_numTasks_])
- **repartition**(_numPartitions_) / **union**(_otherStream_)

- To look up 
	- Config properties
		- While creating spark conf : number of core/thread
	- Examine Kafka Reciever for Input DStream




##### [Window Operations](https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations)
Spark Streaming also provides _windowed computations_, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window. Every time the window _slides_ over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream.This shows that any window operation needs to specify two parameters.

-   _window length_ - The duration of the window (3 in the figure).
-   _sliding interval_ - The interval at which the window operation is performed (2 in the figure).

These two parameters must be multiples of the batch interval of the source DStream (1 in the figure).
![[streaming-dstream-window.png]]

