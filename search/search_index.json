{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Whats_the_plan./","title":"Whats the plan.","text":""},{"location":"Whats_the_plan./#going-on-right-now","title":"Going on right now","text":"<ul> <li>https://www.learnjavaonline.org/en/Arrays</li> <li>To read and complete : https://www.interviewbit.com/java-cheat-sheet/#identifiers-in-java</li> <li></li> </ul>"},{"location":"Blogs/DSALgo-Heuristics/","title":"DSALgo Heuristics","text":"<p>How do I model the information contained in various DS Algo questions , This would require how do I approach a question when I have seen in first place (untagged !! and without any suggestion).</p> <ul> <li>What happens with regular consistent practice ?<ul> <li>A sort of non linear thinking is formed to quickly model the problem's solution into well defined programmable sub steps. Without consistent practice this knowledge cannot be formed or cannot be modelled down into paper, hence </li> </ul> </li> </ul>"},{"location":"Interview/Resources/","title":"Resources","text":""},{"location":"Interview/Resources/#-httpsgithubcomobennerdata-engineering-interview-questionsblobmastercontentkafkamd","title":"- https://github.com/OBenner/data-engineering-interview-questions/blob/master/content/kafka.md","text":""},{"location":"Interview/Spark/","title":"Spark","text":"<p>Topic to prepare - Computation Model (Mop Reduce) -  Execution Model     - Tranformations         -   Narrow : Operations where each input partition contributes to exactly one output partition             -  Map             -  Filter             -  Union         -  Wide : Operations where each input partition contributes to multiple output partitions             -  GroupByKey             -  ReduceByKey             -  Join     - Actions         -  Reduce         -  Count         -  Collect         -  First         -  Take         -  ForEach -  Optimisations     - Code Optimization     - Configuration tuning         - Resource tuning             -  Memory                  - Executor Memory                 - Driver Memory                 - Memory fraction             -      -  Data Structure         - DataFrame         - RDD ( Immutable )         - Datasets     - Algorithm         - Proper algo for distributed computation     - Common performance bottleneck         - Data shuffling : Shuffle is the process of redistributing data across partitions during certain operations like groupBy or join where data needs to be rearranged. Excessive shuffling can impact performance negatively by introducing network overhead, disk I/O, and increased computational complexity.         - Data Skew : Uneven data partition --- Problem             - Partitioning : Proper partitioning is crucial for efficient parallel processing and minimizing data movement during transformations.                 - Different partitioning schemes                     - Hash : Hash function is applied to each record based on a specific key. Useful when you need to group data by a specific key                     - Range : Splits data into ordered ranges. Each partition contains a specified range of data based on a column value. Effective when dealing with sorted or ordered data, such as time-series data.                      - Range based : Round robin partitioning evenly distributes records across partitions without any key-based partitioning. Useful for creating balanced partitions when no specific key is required, such as in parallel processing where record order does not matter. (Shuffling data or using random keys.)                     - Broadca             - Sampling : Identify skewed keys or values using sampling techniques and apply data skew mitigation strategies like replication or redistribution.             - Custom partitioner :  Implement custom partitioners to handle skewed data distribution more efficiently.Example needed             - Salting : Adding a random prefix to keys to distribute skewed data evenly across partitions.         - Inefficient joins : Optimising Joins             -  Choosing Join types : Broadcast join for one of the smaller tables and hash joins for bigger tables             -  Ensuring joined datasets are properly partitioned on the join key.             -  Join predicate should filter data before joining and reduce size before joining             -  Broadcasting small tables to all executors to avoid unnecessary data shuffling         - Resource Contention         - Serialization Overhead         -  Caching(in memory) and persistence(flexible caching) : To cache/persist frequently accessed      -  Monitor and debug performance issues in PySpark         - Spark UI : Monitor resource utilization         - Logging : Enable detailed logging         -  Profiling : Using Sparklens -   Architecture -- Find better content     - ![[0_w5OgGQ0xC9zVoULQ.webp]]     - Driver Program     - Cluster Manager     - Worker Nodes     - Executors -  SparkContext Vs SparkSession -  Spark deployment mode     -  Local Mode : All spark components like driver, executor runs in the same JVM     -  Client Mode :      -  Cluster mode :  -  Streaming     -   Spark Streaming : Works on micro batching,  DStream / Discretized(Powered by RDD) stream. A continious stream of data, We can apply transformations and actions on it.     -  Structured Streaming : Built on Dataframe and Dataser API, can be used with sparkSQL API.         - Differences             - RDD Vs DataStream/Dataset             - Processing with event time, handling late data                 - There is no concept of event time in Spark Streaming, it is present int structured streaming</p>"},{"location":"Notes/Self%20Management/","title":"Self Management","text":"<ul> <li>Threads<ul> <li>Notes</li> <li>Tasks</li> <li>Reminder</li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/101/","title":"101","text":"<p>Major Consumer - Data Analyst - Data Scientist     - ML Models - Other Data Engineer</p> <p>Data modelling - OLTP ( Online transaction processing ) / Relational data modelling / ER Diagrams  - Optimises for low latency, low-volume queries - OLAP ( Online analytical processing ) - Optimises for large Volume, Group By queries, minimises JOINs     - Dimensional modelling     - One Big table - Master Data - Optimises for completeness of entity definition, deduped.</p> <p>Steps -&gt; Prod database snapshot -&gt; Master Data -&gt; Olap Cubes -&gt; Metrics</p> <ul> <li>Cumulative Table Design<ul> <li>2 Dataframes ( yesterday and today )</li> <li>FULL OUTER JOIN the two dataframe together</li> <li>COALESCE values to keep everything around</li> <li>Hang onto all history </li> </ul> </li> <li>Usage<ul> <li>Growth analytics</li> <li>State transition tracking</li> </ul> </li> <li>Strength<ul> <li>Historical analysis without shuffle</li> <li>Easy transition analysis</li> </ul> </li> <li>Drawbacks<ul> <li>Can only be backfilled sequentially </li> <li>Handling PII data can be a mess since deleted/inactive users get carried forward</li> </ul> </li> </ul> <p>![[Screenshot from 2024-05-25 10-02-58.png]] ![[Screenshot from 2024-05-25 11-24-45.png]]</p> <p>CTE / Subqueries / Views - transformation only   Vs Materialised Views - Named query with data  Vs Temp table - Table with data</p>"},{"location":"Notes/Data%20Engineering/Compute/","title":"Compute","text":"<p>Data Processing archiecture</p> <ul> <li>Lambda Architecture</li> <li>Kappa Architecture</li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Stores/","title":"Data Stores","text":"<ul> <li>Relational<ul> <li>MySQL</li> <li>PostGres</li> <li>MariaDB</li> <li>Amazon Arora</li> </ul> </li> <li>Non-Relational<ul> <li>Document<ul> <li>MongoDB</li> <li>ElasticSearch</li> <li>Apache CoucheDB</li> </ul> </li> <li>Wide Column<ul> <li>Apache Aassandra</li> <li>Apache Hbase</li> <li>Google BigTable</li> </ul> </li> <li>Graph<ul> <li>Neo4j</li> <li>DGraph</li> <li>Amazon Neptune</li> </ul> </li> <li>Key-Value<ul> <li>Redis</li> <li>Memcached</li> <li>Amazon DynamoDB</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Reference/","title":"Reference","text":"<p>Modern data engineering : https://data-operating-system.com/ Schema Crawling : https://www.schemacrawler.com/</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Framework%20map/","title":"Framework map","text":"<ul> <li>Cluster resource manager<ul> <li>Hadoop Yarn</li> <li>Apache Mesos</li> <li>Kubernetes</li> </ul> </li> </ul> <p>Flink works on data flow model While Spark works on map reduce model</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Core_Concepts/Data%20Lake/","title":"Data Lake","text":"<p>A <code>single data</code> store for all of the <code>raw data</code> that anyone in an organization might need to analyze -  Martin Fowler</p> <p>Freatures - Schema on read - Structured(Rows and columns)/ Semi-Structured(CSV,XML,JSON) / Unstructured(Documents,emails,PDFs) - Fast ingestion of new data - Data at low level of details</p> <p>General terms - Data Governance     - Orchestration of people, processes, and technology to establish data and information as organization assets         - Policy-level execution, where people from the top-level discovery and design of overall policy take place to handle data         - Implementing, automating, monitoring, and measuring the policy. - Data Ops     -  - Data Catalogue - Data Provenance - Data Lineage     - Captures the origin of data, the sequence of operations associated with data, and status after operation, - </p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Core_Concepts/Data%20Mesh/","title":"Data Mesh","text":"<p>Modern twist - streaming for real-time data availability with architectures such as Kappa - unifying the batch and stream processing for data transformation with frameworks such as Apache Beam, - fully embracing cloud based managed services for storage,</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Core_Concepts/Data%20Mesh/#centralized-and-monolithic-old","title":"Centralized and monolithic (Old)","text":"<ul> <li><code>Ingest data</code> from all corners of the enterprise, ranging from operational and transactional systems and domains that run the business, or external data providers</li> <li><code>Cleanse</code>, enrich, and transform_ the source data into trustworthy data that can address the needs of a diverse set of consumer</li> <li><code>Serve</code> the datasets to a variety of consumers with a diverse set of needs.</li> <li></li> </ul>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Postgres_Architecture/","title":"Postgres Architecture","text":""},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Postgres_Architecture/#postgres-architecture","title":"Postgres architecture","text":"<p>AWS RDS For PostgresSQL , VERSION : 13,12</p> <ul> <li> <p>Replication strategies of interest</p> <ul> <li>Write-Ahead Log Shipping : Warm and hot standby servers can be kept current by reading a stream of write-ahead log (WAL) records. If the main server fails, the standby contains almost all of the data of the main server, and can be quickly made the new primary database server. This can be synchronous or asynchronous and can only be done for the entire database server.A standby server can be implemented using file-based log shipping (Section\u00a027.2) or streaming replication (see Section\u00a027.2.5), or a combination of both. For information on hot standby, see Section\u00a027.4.</li> <li>Logical Replication : Logical replication allows a database server to send a stream of data modifications to another server. PostgreSQL logical replication constructs a stream of logical data modifications from the WAL. Logical replication allows replication of data changes on a per-table basis. In addition, a server that is publishing its own changes can also subscribe to changes from another server, allowing data to flow in multiple directions. For more information on logical replication, see Chapter\u00a031. Through the logical decoding interface (Chapter\u00a049), third-party extensions can also provide similar functionality.</li> </ul> </li> <li> <p>Read replica limitations with PostgreSQL</p> <ul> <li>PostgreSQL read replicas are read-only.</li> <li>Although a read replica isn't a writeable DB instance, you can promote it to become a standalone RDS for PostgreSQL DB instance, it becomes a writable DB instance. It stops receiving write-ahead log (WAL) files from a source DB instance, and it's no longer a read-only instance.</li> <li>You can't create a read replica from another read replica if your RDS for PostgreSQL DB instance is running a PostgreSQL version earlier than 14.1. RDS for PostgreSQL supports cascading read replicas on RDS for PostgreSQL version 14.1 and higher releases only. For more information, see Using cascading read replicas with RDS for PostgreSQL.</li> </ul> </li> <li>Read replica configuration with PostgreSQL<ul> <li>RDS for PostgreSQL uses PostgreSQL native streaming replication to create a read-only copy of a source DB instance. This read replica DB instance is an asynchronously created physical replica of the source DB instance. It's created by a special connection that transmits write ahead log (WAL) data from the source DB instance to the read replica. </li> </ul> </li> <li>How streaming replication works for different RDS for PostgreSQL<ul> <li>A physical replication slot prevents a source DB instance from removing WAL data before it's consumed by all read replicas. Each read replica has its own physical slot on the source DB instance. The slot keeps track of the oldest WAL (by logical sequence number, LSN) that might be needed by the replica. After all slots and DB connections have progressed beyond a given WAL (LSN), that LSN becomes a candidate for removal at the next checkpoint.</li> <li>Amazon RDS uses Amazon S3 to archive WAL data.</li> <li></li> </ul> </li> </ul> <p>#### Enabling CDC with an AWS-managed PostgreSQL DB instance with AWS DMS  AWS DMS supports CDC on Amazon RDS PostgreSQL databases when the DB instance is configured to use logical replication.</p> <p>Referrences - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.ReadReplicas.html - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Flink/Architecture/","title":"Architecture","text":"<p>Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. 1.  Unbounded streams have a start but no defined end. They do not terminate and provide data as it is generated. Unbounded streams must be continuously processed, i.e., events must be promptly handled after they have been ingested. It is not possible to wait for all input data to arrive because the input is unbounded and will not be complete at any point in time. Processing unbounded data often requires that events are ingested in a specific order, such as the order in which events occurred, to be able to reason about result completeness.</p> <ol> <li>Bounded streams have a defined start and end. Bounded streams can be processed by ingesting all data before performing any computations. Ordered ingestion is not required to process bounded streams because a bounded data set can always be sorted. Processing of bounded streams is also known as batch processing.</li> </ol> <p></p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Flink/Architecture/#leverage-in-memory-performance","title":"Leverage In-Memory Performance","text":"<p>Stateful Flink applications are optimized for local state access. Task state is always maintained in memory or, if the state size exceeds the available memory, in access-efficient on-disk data structures. Hence, tasks perform all computations by accessing local, often in-memory, state yielding very low processing latencies. Flink guarantees exactly-once state consistency in case of failures by periodically and asynchronously checkpointing the local state to durable storage.</p> <p></p> <p>Streams can have different characteristics that affect how a stream can and should be processed. -   Bounded and unbounded streams: Streams can be unbounded or bounded, i.e., fixed-sized data sets. Flink has sophisticated features to process unbounded streams, but also dedicated operators to efficiently process bounded streams. -   Real-time and recorded streams: All data are generated as streams. There are two ways to process the data. Processing it in real-time as it is generated or persisting the stream to a storage system, e.g., a file system or object store, and processed it later. Flink applications can process recorded or real-time streams.</p> <p>State Every non-trivial streaming application is stateful,Any application that runs basic business logic needs to remember events or intermediate results to access them at a later point in time, for example when the next event is received or after a specific time duration.</p> <p></p> <ul> <li>Multiple State Primitives: Flink provides state primitives for different data structures, such as atomic values, lists, or maps.</li> <li>Pluggable State Backends: Application state is managed in and checkpointed by a pluggable state backend.</li> <li>Exactly-once state consistency: Flink\u2019s checkpointing and recovery algorithms guarantee the consistency of application state in case of a failure.</li> <li>Very Large State: Flink is able to maintain application state of several terabytes in size due to its asynchronous and incremental checkpoint algorithm.</li> <li>Scalable Applications: Flink supports scaling of stateful applications by redistributing the state to more or fewer workers.</li> </ul> <p>Time Most event streams have inherent time semantics because each event is produced at a specific point in time. Moreover, many common stream computations are based on time, such as windows aggregations, sessionization, pattern detection, and time-based joins. - Event-time Mode: Applications that process streams with event-time semantics compute results based on timestamps of the events - Watermark Support: Flink employs watermarks to reason about time in event-time applications. - Late Data Handling: Flink features multiple options to handle late events, such as rerouting them via side outputs and updating previously completed results. - Processing-time Mode: </p> <p>Layered APIs</p> <p></p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/","title":"Spark Streaming (DStream)","text":"<p> Spark streaming enables sclable, high-throughput, fault-tolerant stream processing of live data streams. Spark streaming provides a high level abstraction calles discretized stream or DStream, which represents a continious stream of data.</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#discretized-streams-dstreams","title":"Discretized Streams DStreams","text":"<p>Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark\u2019s abstraction of an immutable, distributed dataset  </p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#input-dstreams-and-receivers","title":"Input DStreams and Receivers","text":"<p>Input DStreams are DStreams representing the stream of input data received from streaming source.. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark\u2019s memory for processing.</p> <p>Spark Streaming provides two categories of built-in streaming sources.</p> <ul> <li>Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.</li> <li>Advanced sources: Sources like Kafka, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.</li> </ul>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#receiver-reliability","title":"Receiver Reliability","text":"<p>There can be two kinds of data sources based on their reliability. Sources (like Kafka) allow the transferred data to be acknowledged. 1.  Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication. 2.  Unreliable Receiver - An unreliable receiver does not send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#transformation","title":"Transformation","text":"<ul> <li>map(func) / flatMap(func)</li> <li>filter(func) join(otherStream, [numTasks])</li> <li>count() / countByValue()</li> <li>reduce(func) / reduceByKey(func, [numTasks]) / transform(func) / updateStateByKey(func)</li> <li>cogroup(otherStream, [numTasks])</li> <li> <p>repartition(numPartitions) / union(otherStream)</p> </li> <li> <p>To look up </p> <ul> <li>Config properties<ul> <li>While creating spark conf : number of core/thread</li> </ul> </li> <li>Examine Kafka Reciever for Input DStream</li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#window-operations","title":"Window Operations","text":"<p>Spark Streaming also provides windowed computations, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window. Every time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream.This shows that any window operation needs to specify two parameters.</p> <ul> <li>window length - The duration of the window (3 in the figure).</li> <li>sliding interval - The interval at which the window operation is performed (2 in the figure).</li> </ul> <p>These two parameters must be multiples of the batch interval of the source DStream (1 in the figure). </p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#join-operations","title":"Join Operations","text":"<p>You can perform different kinds of joins in Spark Streaming.(<code>leftOuterJoin</code>, <code>rightOuterJoin</code>, <code>fullOuterJoin</code>).</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#caching-persistence","title":"Caching / Persistence","text":"<p>This is useful if the data in the DStream will be computed multiple times (e.g., multiple operations on the same data). For window-based operations like <code>reduceByWindow</code> and <code>reduceByKeyAndWindow</code> and state-based operations like <code>updateStateByKey</code>, this is implicitly true.</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Spark_Streaming_%28DStream%29/#checkpointing","title":"Checkpointing","text":"<p>Spark Streaming needs to checkpoint enough information to a fault- tolerant storage system such that it can recover from failures. There are two types of data that are checkpointed.</p> <ul> <li>Metadata checkpointing - Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later). Metadata includes:<ul> <li>Configuration - The configuration that was used to create the streaming application.</li> <li>DStream operations - The set of DStream operations that define the streaming application.</li> <li>Incomplete batches - Batches whose jobs are queued but have not completed yet.</li> </ul> </li> <li>Data checkpointing - Saving of the generated RDDs to reliable storage. This is necessary in some stateful transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically checkpointed to reliable storage (e.g. HDFS) to cut off the dependency chains.</li> </ul>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Structured_Streaming./","title":"Structured Streaming.","text":"<p>Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs.</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Structured_Streaming./#programming-model","title":"Programming Model","text":"<p>Note that Structured Streaming does not materialize the entire table. It reads the latest available data from the streaming data source, processes it incrementally to update the result, and then discards the source data. It only keeps around the minimal intermediate state data as required to update the result</p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Structured_Streaming./#window-operations-on-event-time","title":"Window Operations on Event Time","text":"<p>Aggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations. In a grouped aggregation, aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column. In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into. </p>"},{"location":"Notes/Data%20Engineering/Architecture_and_Principles/Elements/Spark/Structured_Streaming./#handling-late-data-and-watermarking","title":"Handling Late Data and Watermarking","text":"<p>Now consider what happens if one of the events arrives late to the application. For example, say, a word generated at 12:04 (i.e. event time) could be received by the application at 12:11. The application should use the time 12:04 instead of 12:11 to update the older counts for the window <code>12:00 - 12:10</code>. This occurs naturally in our window-based grouping \u2013 Structured Streaming can maintain the intermediate state for partial aggregates for a long period of time such that late data can update aggregates of old windows correctly.  However, to run this query for days, it\u2019s necessary for the system to bound the amount of intermediate in-memory state it accumulates. This means the system needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more. Watermarking, which lets the engine automatically track the current event time in the data and attempt to clean up old state accordingly. You can define the watermark of a query by specifying the event time column and the threshold on how late the data is expected to be in terms of event time. For a specific window ending at time <code>T</code>, the engine will maintain state and allow late data to update the state until <code>(max event time seen by the engine - late threshold &gt; T)</code>. In other words, late data within the threshold will be aggregated, but data later than the threshold will start getting dropped </p>"},{"location":"Notes/Data%20Engineering/CDC/101/","title":"101","text":"<ul> <li>Core components of Postgres used for CDC</li> <li>How Debezium used the components</li> <li>Deployment strategy <ul> <li>Debezium</li> <li>Postgres<ul> <li>Parameter tweaks</li> </ul> </li> </ul> </li> </ul> <p>WAL Logical Replication Publication</p>"},{"location":"Notes/Data%20Engineering/CDC/Postgres%20Core%20components/","title":"Postgres Core components","text":"<p>One aspect of reliable operation is that all data recorded by a committed transaction should be stored in a nonvolatile area that is safe from power loss, operating system failure, and hardware failure.wal_sync_method to adjust how force writes from buffer cache to disk.  \u00a0 \u00a0WAL's central concept is that changes to data files (where tables and indexes reside) must be written only after those changes have been logged, that is, after WAL records describing the changes have been flushed to permanent storage. If we follow this procedure, we do not need to flush data pages to disk on every transaction commit, because we know that in the event of a crash we will be able to recover the database using the log: any changes that have not been applied to the data pages can be redone from the WAL records.\u00a0(This is roll-forward recovery, also known as REDO.)</p> <p>Checkpoints\u00a0are points in the sequence of transactions at which it is guaranteed that the heap and index data files have been updated with all information written before that checkpoint. At checkpoint time, all dirty data pages are flushed to disk and a special checkpoint record is written to the WAL file.In the event of a crash, the crash recovery procedure looks at the latest checkpoint record to determine the point in the WAL (known as the redo record) from which it should start the REDO operation.A checkpoint is begun every\u00a0checkpoint_timeout\u00a0seconds, or if\u00a0max_wal_size\u00a0is about to be exceeded, whichever comes first. - Parameters     - checkpoint_timeout     - max_wal_size     - archive_timeout : WAL archiving is being used and you want to put a lower limit on how often files are archived in order to bound potential data loss.</p> <p>Asynchronous commit is an option that allows transactions to complete more quickly, at the cost that the most recent transactions may be lost if the database should crash.\u00a0Selecting asynchronous commit mode means that the server returns success as soon as the transaction is logically completed, before the\u00a0WAL\u00a0records it generated have actually made their way to disk.</p>"},{"location":"Notes/Data%20Engineering/Data%20Culture/What%20is%20the%20standard%20of%20a%20data%20driven%20organization%20%3F/","title":"What is the standard of a data driven organization ?","text":"<p>Let us start us on the way to answering our first question: what does it mean for an organization to be data-driven? * Prerequisite #1: An organization must be collecting data. * Prerequisite #2: Data must be accessible and queryable.     * Joinable : The data must be in a form that can be joined to other enterprise data when necessary.e.g. : There must be a data-sharing culture within the organization so that data can be joined, such as combining customers\u2019 clickstream with their transactional history.     * Shareable : Siloed data is always going to inhibit the scope of what can be achieved. When more data is available to more parts of a system, the whole is greater than the sum of the parts.     * Queryable : There must be appropriate tools to query and slice and dice the data. All reporting and analysis requires filtering, grouping, and aggregating data to reduce the large amounts of raw data into a smaller set of higher-level numbers that help our brains comprehend what is happening in a business.</p> <p>Analytical Functions * Reporting     * Reports     * Alerts     * Dashboards * Analysis</p> Time Past Present Future Information a) What happened ? b)What is happening c) What will happen ? Reporting Alerts Extrapolation Insight d)How and why did it happen ? e)What is the next best action f) What is the best/ worst can happen ? Modelling, Experimentation, Recommendation Prediction, optimization Design simulation <p>Hallmarks of Data-Drivenness * A data-driven organization will almost certainly be choosing among future options or actions using a suite of weighted variables. Resources are always finite, and there are always pros and cons for different reasonable courses of action. One should gather data for each of the set of variables that are of concern or interest and determine weights among those to generate a final leading decision. * A data-driven organization may be continuously testing. It might be A/B testing checkout flow on a website or testing email subject lines in a marketing campaign. * A data-driven organization may be involved in predictive modeling, forecasting sales, stock prices, or company revenue, but importantly feeding the prediction errors and other learning back into the models to help improve them.</p> <p>Treat data as first class citizer, Data must be presented to influence the Decision maker (No Decisions from guts should be taken). Data has to drive reports leading to deeper dives and analysis. A data-driven organisation will use data as critical evidence to help inform and influence strategy. </p> <p></p> <p></p>"},{"location":"Notes/Data%20Engineering/Data%20Governance/Elements%20of%20Data%20Governance/","title":"Elements of Data Governance","text":"<p>Tools * Schema Crawling at source : https://www.schemacrawler.com/code-examples.html * </p>"},{"location":"Notes/Data%20Engineering/Data%20Lakes/Hudi/Concepts/","title":"Concepts","text":""},{"location":"Notes/Data%20Engineering/Data%20Lakes/Hudi/Concepts/#what-is-hudi","title":"What is Hudi ?","text":"<ul> <li>What it provides<ul> <li>Table abstraction on data lake</li> <li>Transaction </li> <li>Efficient upserts.deletes</li> <li>Advanced Indexes</li> <li>Streaming Ingestion</li> <li>Data compaction/clustering </li> <li>Concurrency</li> <li></li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Lakes/Hudi/Concepts/#concepts","title":"Concepts","text":"<ul> <li>Timeline<ul> <li>At its core, Hudi maintains a <code>timeline</code> of all actions performed on the table at different <code>instants</code>, A Hudi instant consists of the following components<ul> <li><code>Instant action</code> : Type of action performed on the table</li> <li><code>Instant time</code> : Instant time is typically a timestamp (e.g: 20190117010349), which monotonically increases in the order of action's begin time.</li> <li><code>state</code> : current state of the instant</li> </ul> </li> <li>Hudi guarantees that the actions performed on the timeline are atomic &amp; timeline consistent based on the instant time,Key action performed include<ul> <li><code>COMMITS</code> - A commit denotes an atomic write of a batch of records into a table.</li> <li><code>CLEANS</code> - Background activity that gets rid of older versions of files in the table, that are no longer needed.</li> <li><code>DELTA_COMMIT</code> - A delta commit refers to an atomic write of a batch of records into a MergeOnRead type table, where some/all of the data could be just written to delta logs.</li> <li><code>COMPACTION</code> - Background activity to reconcile differential data structures within Hudi e.g: moving updates from row based log files to columnar formats. Internally, compaction manifests as a special commit on the timeline</li> <li><code>ROLLBACK</code> - Indicates that a commit/delta commit was unsuccessful &amp; rolled back, removing any partial files produced during such a write</li> <li><code>SAVEPOINT</code> - Marks certain file groups as \"saved\", such that cleaner will not delete them. It helps restore the table to a point on the timeline, in case of disaster/data recovery scenarios.</li> </ul> </li> <li>Any given instant can be in one of the following states<ul> <li><code>REQUESTED</code> - Denotes an action has been scheduled, but has not initiated</li> <li><code>INFLIGHT</code> - Denotes that the action is currently being performed</li> <li><code>COMPLETED</code> - Denotes completion of an action on the timeline</li> </ul> </li> </ul> </li> <li>Table &amp; Query Types : Hudi table types define how data is indexed &amp; laid out on the DFS and how the above primitives and timeline activities are implemented on top of such organization (i.e how data is written). In turn, <code>query types</code> define how the underlying data is exposed to the queries (i.e how data is read).<ul> <li>Table type<ul> <li>COW (Copy on write) : File slices in Copy-On-Write table only contain the base/columnar file and each commit produces new versions of base files. In other words, we implicitly compact on every commit, such that only columnar data exists. As a result, the write amplification (number of bytes written for 1 byte of incoming data) is much higher, where read amplification is zero. This is a much desired property for analytical workloads, which is predominantly read-heavy.As data gets written, updates to existing file groups produce a new slice for that file group stamped with the commit instant time, while inserts allocate a new file group and write its first slice for that file group.</li> <li></li> <li>MOR (Merge On Read) : Stores data using a combination of columnar (e.g parquet) + row based (e.g avro) file formats. Updates are logged to delta files &amp; later compacted to produce new versions of columnar files synchronously or asynchronously .Merge on read table is a superset of copy on write, in the sense it still supports read optimized queries of the table by exposing only the base/columnar files in latest file slices. Additionally, it stores incoming upserts for each file group, onto a row based delta log, to support snapshot queries by applying the delta log, onto the latest version of each file id on-the-fly during query time. Thus, this table type attempts to balance read and write amplification intelligently, to provide near real-time data. The most significant change here, would be to the compactor, which now carefully chooses which delta log files need to be compacted onto their columnar base file, to keep the query performance in check (larger delta log files would incur longer merge times with merge data on query side)</li> <li></li> </ul> </li> <li>Query Type<ul> <li>Snapshot Queries : Queries see the latest snapshot of the table as of a given commit or compaction action. In case of merge on read table, it exposes near-real time data(few mins) by merging the base and delta files of the latest file slice on-the-fly. For copy on write table, it provides a drop-in replacement for existing parquet tables, while providing upsert/delete and other write side features.</li> <li>Incremental Queries : Queries only see new data written to the table, since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines.</li> <li>Read Optimized Queries : Queries see the latest snapshot of table as of a given commit/compaction action. Exposes only the base/columnar files in latest file slices and guarantees the same columnar query performance compared to a non-hudi columnar table.</li> </ul> </li> </ul> </li> <li>Indexing : Hudi provides efficient upserts, by mapping a given hoodie key (record key + partition path) consistently to a file id, via an indexing mechanism. This mapping between record key and file group/file id, never changes once the first version of a record has been written to a file. In short, the mapped file group contains all versions of a group of records.<ul> <li>Index types<ul> <li>Bloom Index (default): Employs bloom filters built out of the record keys, optionally also pruning candidate files using record key ranges.</li> <li>Simple Index: Performs a lean join of the incoming update/delete records against keys extracted from the table on storage.</li> <li>HBase Index: Manages the index mapping in an external Apache HBase table.</li> <li>Bring your own implementation: You can extend this public API to implement custom indexing.</li> </ul> </li> <li>Another key aspect worth understanding is the difference between global and non-global indexes.<ul> <li>Global indexes enforce uniqueness of keys across all partitions of a table i.e guarantees that exactly one record exists in the table for a given record key. Global indexes offer stronger guarantees, but the update/delete cost grows with size of the table <code>O(size of table)</code>, which might still be acceptable for smaller tables.</li> <li>Non Global Index is the default index implementations enforce this constraint only within a specific partition. As one might imagine, non global indexes depends on the writer to provide the same consistent partition path for a given record key during update/delete, but can deliver much better performance since the index lookup operation becomes <code>O(number of records updated/deleted)</code> and scales well with write volume.</li> </ul> </li> </ul> </li> <li>File layout<ul> <li>Hudi organizes data tables into a directory structure under a base path on a distributed file system</li> <li>Tables are broken up into partitions</li> <li>Within each partition, files are organized into file groups, uniquely identified by a file ID</li> <li>Each file group contains several file slices</li> <li>Each slice contains a base file (.parquet) produced at a certain commit/compaction instant time, along with set of log files (.log.*) that contain inserts/updates to the base file since the base file was produced.</li> </ul> </li> <li>Metadata Table, the main purpose of the Metadata Table is to eliminate the requirement for the \"list files\" operation.</li> <li>Multi-modal index can drastically improve the lookup performance in file index and query latency with data skipping.</li> <li>Write Operations<ul> <li>UPSERT : This is the default operation where the input records are first tagged as inserts or updates by looking up the index, this operation is recommended for use-cases like database change capture where the input almost certainly contains updates. The target table will never show duplicates.</li> <li>INSERT : This operation is very similar to upsert in terms of heuristics/file sizing but completely skips the index lookup step.</li> <li>BULK_INSERT : For initial loading/bootstrapping a Hudi table at first.</li> <li>DELETE : Hudi supports implementing two types of deletes on data stored in Hudi tables, by enabling the user to specify a different record payload implementation.<ul> <li>Soft Deletes : Retain the record key and just null out the values for all the other fields. This can be achieved by ensuring the appropriate fields are nullable in the table schema and simply upserting the table after setting these fields to null.</li> <li>Hard Deletes : A stronger form of deletion is to physically remove any trace of the record from the table. This can be achieved in 3 different ways.<ul> <li>Using DataSource, set <code>OPERATION_OPT_KEY</code> to <code>DELETE_OPERATION_OPT_VAL</code>. This will remove all the records in the DataSet being submitted.</li> <li>Using DataSource, set <code>PAYLOAD_CLASS_OPT_KEY</code> to <code>\"org.apache.hudi.EmptyHoodieRecordPayload\"</code>. This will remove all the records in the DataSet being submitted.</li> <li>Using DataSource or DeltaStreamer, add a column named <code>_hoodie_is_deleted</code> to DataSet. The value of this column must be set to <code>true</code> for all the records to be deleted and either <code>false</code> or left null for any records which are to be upserted</li> </ul> </li> </ul> </li> <li>Write Sequence<ul> <li> <ol> <li>Deduping</li> <li>First your input records may have duplicate keys within the same batch and duplicates need to be combined or reduced by key.</li> </ol> </li> <li>Index Lookup<ol> <li>Next, an index lookup is performed to try and match the input records to identify which file groups they belong to.</li> </ol> </li> <li>File Sizing<ol> <li>Then, based on the average size of previous commits, Hudi will make a plan to add enough records to a small file to get it close to the configured maximum limit.</li> </ol> </li> <li>Partitioning<ol> <li>We now arrive at partitioning where we decide what file groups certain updates and inserts will be placed in or if new file groups will be created</li> </ol> </li> <li>Write I/O<ol> <li>Now we actually do the write operations which is either creating a new base file, appending to the log file, or versioning an existing base file.</li> </ol> </li> <li>Update Index<ol> <li>Now that the write is performed, we will go back and update the index.</li> </ol> </li> <li>Commit<ol> <li>Finally we commit all of these changes atomically. (A callback notification is exposed)</li> </ol> </li> <li>Clean (if needed)<ol> <li>Following the commit, cleaning is invoked if needed.</li> </ol> </li> <li>Compaction<ol> <li>If you are using MOR tables, compaction will either run inline, or be scheduled asynchronously</li> </ol> </li> <li>Archive<ol> <li>Lastly, we perform an archival step which moves old timeline items to an archive folder.</li> </ol> </li> </ul> </li> </ul> </li> <li>Schema evolution<ul> <li>Scenarios<ol> <li>Columns (including nested columns) can be added, deleted, modified, and moved.</li> <li>Partition columns cannot be evolved.</li> <li>You cannot add, delete, or perform operations on nested columns of the Array type.</li> </ol> </li> </ul> </li> <li>Key Generation<ul> <li>Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where the record belongs to. Using primary keys, Hudi can impose<ul> <li>Partition level uniqueness integrity constraint</li> <li>Enable fast updates and deletes on records</li> </ul> </li> <li>have        </li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Lakes/Hudi/Hudi%20Elements/","title":"Hudi Elements","text":"<ul> <li>Configuration to choose</li> <li>Deployment consideration<ul> <li>https://hudi.apache.org/docs/metadata#deployment-considerations</li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Modelling/101/","title":"101","text":""},{"location":"Notes/Data%20Engineering/Data%20Modelling/101/#what-is-data-modelling","title":"What is data modelling ?","text":"<p>-&gt;  Well defined data models to store the data in a database (data warehouse) depending on the business requirements (mostly OLAP based systems)</p>"},{"location":"Notes/Data%20Engineering/Data%20Modelling/101/#factor-considering-while-data-modelling","title":"Factor considering while data modelling","text":"<ul> <li>Scope of business : There are several departments and diverse business functions around.</li> <li>Feasibility of the data granularity levels of filtering, aggregation, slicing, and dicing</li> <li>Starts with logical modelling across multi-platforms and an extensive-architecture approach, its enhanced performance, and scalability.</li> <li>Serving data for all types and different categories of consumers<ul> <li>[Data Scientist, Data Analysts, Downstream applications, API-based system, Data Sharing systems]</li> </ul> </li> <li>Highly flexible deployment and decoupling approach for cost-effectiveness.</li> <li>Well-defined Data Governance model to support quality, visibility, availability security</li> <li>Streamlined Master Data Management and Data Catalogue and Curation to support functionally and technically.</li> <li>Perfect monitoring and tracking of the Data Linage from Source into Serving layer</li> <li>Ability to facilitate Batch, Real-Time analysis, and Lambda process of high-velocity, verity, and veracity data.</li> <li>Supports Analytics and Advanced Analytics components.</li> <li>Agile Delivery approach from Data modelling and delivering aspects to satisfy, their business model.</li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Modelling/101/#stages-of-data-modelling","title":"Stages of data modelling","text":"<p>-&gt; It comprises of 3 stages, conceptual, logical and physical ,</p>"},{"location":"Notes/Data%20Engineering/Data%20Modelling/101/#modelling-techniques","title":"Modelling techniques","text":"<ul> <li>Relational data modelling</li> <li>Dimensional data modelling</li> <li>Data Vault modelling</li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Modelling/101/#dimensional-data-modelling","title":"Dimensional Data Modelling","text":"<ul> <li>What is it ?</li> <li>Techniques<ul> <li>Star Schema</li> <li>Snowflake Schema</li> </ul> </li> <li>Approach<ul> <li>Top-down approach ~ Coined by Inmon</li> <li>Bottom-up approach ~ Coined by Kimball</li> </ul> </li> </ul> <p>Data Mart</p> <p>-&gt; ETL and ELT - ETL     - Data is extracted, transformed and loaded into warehouse, schema has to be enforced     - Two types         - Full Load/ETL         - Incremental/Delta ETL  - ELT     - </p> <p>Schema on Read Vs Schema on Write</p> <p>Dimensional Modelling - Fact table : Quantitative data related to business, For eg : In sales data scenario, fact table will have measures like sales amount, quantity sold, and profit and the foreign keys in the fact table would link to dimension tables such as data, product, and store etc.     - Fact hold 2 type of data         - Measures         - Foreign key of dimension     - Grain of fact table     - Types of fact table         - Transaction fact table :  This captures the individual business events or transactions at a detailed level.For eg :  a sale, transaction, customer order, service request etc         - Periodic snapshot fact table : Stores aggregated data at specific interval or snapshots like daily, weekly, monthly. It represents the state of metrics or KPIs at a specific point in time providing a snapshot of business performance over time. Usually used for trend analysis.         - Accumulating snapshot fact table : Tracks the progress of a process over time, it stores data related to specific process stages, and as the process progresses, new information is added to the fact table to reflect the current state. Usually used in process-oriented analysis.         - Fact-less fact table : These contains references to foreign keys only without any numeric measure. For example : Enrolment data : ( student_id , course_id , enrolment_date ) - It doesn't have any numeric measure - Dimension table (Attributes) : Stores descriptive or textual data about business entities referred to as dimensions.They help organise and provide context to the measures or facts stored in fact tables.     - Key characteristics         - Descriptive attributes         - Primary Key         - No numeric data         - Hierarchical Structure         - Low cardinality     - Types of dimension table         - Conformed dimension : A dimension that has consistent and uniform attributed and definitions across all data marts within an organisation. It can be used seamlessly in different parts of a data warehouse. For eg : Date (Date,month,day,year,hour,minute,second,day_of_week,public_holiday,fiscal_period)         - Junk Dimension : It consolidates multiple low-cardinality flags or attributed into a single table. For eg : Lets say there are 3 low cardinality dimension              - Promotion type : (discount, free_gift , buy_one_get_one)             - Payment method : (cash, credit_card, debit_rard)             - Purchase channel : (in_store, online, mobile_app)                 - Instead of creating separate dimension table for each of these attributes, we can create a junk dimension                     - junk_dimension                         - junk_key                         - promotion_type                         - payment_method                         - purchase_channel         -  Degenerate dimension : It is derived from a fact table and does not have separate dimension table, used for grouping and aggregating facts.  For eg :  A sale fact table has the following columns             - transaction_id ( degenerate dimension ) : a unique identifier for each sales transaction             - product_key ( foreign key )             - date_key ( foreign key )             - sales_amount             - quantity_sold         - Role playing dimension : Role playing dimensions are dimensions that are used more than once in a fact table, each time with a different meaning or role. For eg : An order fact table with following columns, Here DateKey dimension is a role playing dimension             - Order_date ( DateKey )             - Ship_date ( DateKey )             - Ship_date ( DateKey )             - order_id             - product_id             - amount         -  Slowly changing dimension : Slowly changing dimensions refer to how data in your data warehouse changes over time. Slowly changing dimensions have the same natural key but other data columns that may or may not change over time depending on the type of dimensions that it is.             - Type 0 : Dimensions that never change. Like States, Date, Zipcode etc,              - Type 1 : Type 1 refers to data that is overwritten by new data without keeping a historical record of that old piece of data.             - Type 2 :  Type 2 dimensions are always created as a new record. If a detail in the data changes, a new row will be added to the table with a new primary key. However, the natural key would remain the same in order to map a record change to one another. Type 2 dimensions are the most common approach to tracking historical records.             - Type 3 : Type 3 dimensions track changes in a row by adding a new column. Instead of adding a new row with a new primary key like with type 2 dimensions, the primary key remains the same and an additional column is appended. This is good if you need your primary key ro remain unique and only have one record for each natural key. However, you can really only track one change in a record rather than multiple changes over time. E.g : et\u2019s say your warehouse location is changing. Because you don\u2019t expect the address of your warehouse to change more than once, you add a <code>current_address</code> column with the address of your new warehouse. You then change the original address column name to be <code>previous_address</code> and store your old address information.             - Type 4 : Type 4 dimensions exist as records in two different tables- a current record table and a historical record table. All of the records that are active in a given moment will be in one table and then all of the records considered historical will exist in a separate history table. This is a great way of keeping track of records that have many changes over time.</p> <p>Reference :   * https://www.couchbase.com/blog/conceptual-physical-logical-data-models/  * https://bennyaustin.com/2010/05/02/kimball-and-inmon-dw-models/  * https://www.1keydata.com/datawarehousing/datawarehouse.html</p> <p>Videos * https://www.youtube.com/watch?v=7JbCVXmJ1bs</p> <p>Projects - https://github.com/awslabs/deequ -  Unit Tests for Data - https://greatexpectations.io/ - A powerful platform to uphold data quality</p>"},{"location":"Notes/Data%20Engineering/Data%20Platform/Apache%20Druid/","title":"Apache Druid","text":"<ul> <li>Good Choices for  druid<ul> <li>Insert rates are very high, but updates are less common.</li> <li>You may have more than one table, but each query hits just one big distributed table. Queries may potentially hit more than one smaller \"lookup\" table.</li> <li>You have high cardinality data columns, e.g. URLs, user IDs, and need fast counting and ranking over them.</li> <li>Your data has a time component. Druid includes optimizations and design choices specifically related to time. </li> <li>Most of your queries are aggregation and reporting queries. For example \"group by\" queries. You may also have searching and scanning queries.</li> </ul> </li> <li>Situations where you would likely not want to use Druid include:<ul> <li>You need low-latency updates of existing records using a primary key. Druid supports streaming inserts, but not streaming updates. You can perform updates using background batch jobs.</li> <li>Big Long Running queries<ul> <li>Big Reporting queries.</li> <li>Big Joins</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Platform/Apache%20Druid/#design","title":"Design","text":"<ul> <li>DataSources : Druid data is stored in datasources, which are similar to tables in a traditional RDBMS. Each datasource is partitioned by time and, optionally, further partitioned by other attributes. Each time range is called a chunk (for example, a single day, if your datasource is partitioned by day).Each segment is created by a MiddleManager as mutable and uncommitted. Data is queryable as soon as it is added to an uncommitted segment.</li> </ul>"},{"location":"Notes/Data%20Engineering/Data%20Platform/Architecture/","title":"Architecture","text":"<p>The data should have all the answers.</p> <p>To create a platform to consume and produce data to all sources , basically to make all possible queryable, th</p> <p>Key Aspects Of Data Platform * Enable self-service for a diverse range of users     *  If someone wants to bring data into their work, they should be able to easily find the data they need.         * Easily discover and analyze data within the platform         * Understand the context associated with data, such as column descriptions, history and lineage         * Derive insights from data with minimal dependencies on the data and Analytics Team * Enable Agile Data Management     * Availability: Data is already available in a data lake or warehouse. Modern data lakes and warehouses separate storage and compute, which makes it possible to store large amounts of data for relatively cheap.     * Elasticity: Compute is based on a cloud platform, which allows for elasticity and auto-scalability. For example, if end users consume the most data and analytics on Friday afternoons, it should be possible to auto-scale processing power on Friday afternoon to give users a great experience and then scale down within a few hours. * </p> <p>Analytics Workloads * Traditional Analytics * Search Based Analytics * Predictive Analytics * Ad-Hod Analytics</p> <p>Business Intelegence * Reports * Dashboards * Embedded Analytics * Self-Service BI</p> <p>Referrences * https://www.analyticsvidhya.com/blog/2022/01/layers-of-the-data-platform-architecture/ * </p>"},{"location":"Notes/Data%20Engineering/Data%20Platform/Data%20Platform/","title":"Data Platform","text":"<p>What is it ?     A modern data platform is a collection of tools and capabilities that, when brought together, allow organizations to achieve the gold standard \u2014 a fundamentally data-driven organization.</p> <p>Evolution  Current Roles and Responsibilities Elements of a data platform and their general implementation</p> <p>Referrences * https://towardsdatascience.com/the-building-blocks-of-a-modern-data-platform-92e46061165 * </p>"},{"location":"Notes/Data%20Engineering/Data%20Platform/EMR_Jupyter/","title":"EMR Jupyter","text":"<p>Spark EMR Config  * Spark Settings     * <code>maximizeResourceAllocation</code> : True or False, default : False         * Settings configured in <code>spark-defaults</code> when <code>maximizeResourceAllocation</code> is enabled             * spark.default.parallelism                 * Description : Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user.                 * Value : 2X number of CPU cores available to YARN containers.             * spark.driver.memory                 * Description : Amount of memory to use for the driver process, i.e. where SparkContext is initialized. (for example, 1g, 2g).                 * Value : Setting is configured based on the instance types in the cluster             * spark.executor.memory                 * Description : Amount of memory to use per executor process.                 * Value : Setting is configured based on the core and task instance types in the cluster.             * spark.executor.cores                 * Description : The number of cores to use on each executor.                 * Value : Setting is configured based on the core and task instance types in the cluster.             * spark.executor.instances                 * Description : The number of executors.                 * Value : Setting is configured based on the core and task instance types in the cluster.     * Spark defaults set by Amazon EMR - No need to configure         * spark.executor.memory : Setting is configured based on the core and task instance types in the cluster.(Amount of memory to use per executor process. (for example, 1g, 2g))         * spark.executor.cores :  Setting is configured based on the core and task instance types in the cluster.(number of cores to use on each executor)         * spark.dynamicAllocation.enabled : true (emr-4.4.0 or greater),Whether to use dynamic resource allocation, which scales the number of executors registered with an application up and down based on the workload.     * Configuring node decommissioning behaviour : Default value         *  spark.blacklist.decommissioning.enabled : true         * spark.blacklist.decommissioning.timeout : 1h         * spark.decommissioning.timeout.threshold : 20s         * spark.resourceManager.cleanupExpiredHost : true         * spark.stage.attempt.ignoreOnDecommissionFetchFailure : true</p> <p>Following cluster configurations when you work with EMR Studio. * Livy     <pre><code>{\n    \"classification\":\"livy-conf\",\n        \"Properties\":{\n            \"livy.server.session.timeout\":\"6h\",\n            \"livy.spark.deploy-mode\":\"cluster\"\n        }\n}\n</code></pre> * Set deploy mode for Spark sessions to cluster mode * Use memory optimised instances, Eg : r5.2x, r5.4x, r5.8x, r5.12x, r5.16x, r4.2x, r4.4x, r4.8x, r4.12, * Use the capacity-optimized allocation strategy for Spot Instances to help Amazon EMR make effective instance selections based on real-time capacity insights from Amazon EC2. For more information, see Allocation strategy for instance fleets. * </p>"},{"location":"Notes/Data%20Engineering/Data%20Platform/Real%20time%20Analytics/","title":"Real time Analytics","text":"<p>Fast Slice and dice Real time metrics</p> <p>Druid : Real-time analytics database designed for fast slice-and-dice analytics Needs : Event Oriented data</p>"},{"location":"Notes/Data%20Engineering/SQL/101/","title":"101","text":"<p>Joins</p> <p>Aggregates</p> <p>Window functions : A window function makes a calculation across multiple rows that are related to the current row.Similar to an aggregate function (<code>GROUP BY</code>), a window function performs the operation across multiple rows. Unlike an aggregate function, a window function does not group rows into one single row. For eg - Running total - 7-day moving average. - Rankings</p> <p>Sample expression <pre><code>  SELECT\n    window_function() OVER(\n         PARTITION BY partition_expression\n         ORDER BY order_expression\n         window_frame_extent\n    ) AS window_column_alias\n    FROM table_name\n</code></pre></p> <ul> <li>Order By : It changes the basis on which the function assigns numbers to rows. For example, if we\u00a0<code>ORDER BY</code>\u00a0the expression\u00a0<code>price</code>\u00a0on an ascending order, then the lowest-priced item will have the lowest rank.</li> <li>Partition By : While the result of a\u00a0<code>GROUP BY</code>\u00a0aggregates all rows, the result of a window function using\u00a0<code>PARTITION BY</code>\u00a0aggregates each partition independently. Without the\u00a0<code>PARTITION BY</code>\u00a0clause, the result set is one single partition.</li> <li>Window Frame Extend : A window frame is the selected set of rows in the partition over which aggregation will occur. Put simply, they are a set of rows that are somehow related to the current row.A window frame is defined by a lower bound and an upper bound relative to the current row. The lowest possible bound is the first row, which is known as\u00a0<code>UNBOUNDED PRECEDING</code>. The highest possible bound is the last row, which is known as\u00a0<code>UNBOUNDED FOLLOWING</code>. For example, if we only want to get 5 rows before the current row, then we will specify the range using\u00a0<code>5 PRECEDING</code>.</li> </ul> Function Syntax Function Description Additional Notes <code>ROW_NUMBER()</code> Assigns a sequential integer to each row within the partition of a result set. Row numbers are not repeated within each partition. <code>RANK()</code> Assigns a rank number to each row in a partition. - Tied values are given the same rank.- The next rankings are skipped. <code>DENSE_RANK()</code> Same as rank but , next rankings are not skipped Example <code>PERCENT_RANK()</code> Assigns the rank number of each row in a partition as a percentage. - Tied values are given the same rank.- Computed as the fraction of rows less than the current row, i.e., the rank of row divided by the largest rank in the partition. <code>NTILE(n_buckets)</code> Distributes the rows of a partition into a specified number of buckets. - For example, if we perform the window function NTILE(5) on a table with 100 rows, they will be in bucket 1, rows 21 to 40 in bucket 2, rows 41 to 60 in bucket 3, et cetera. <code>CUME_DIST()</code> The cumulative distribution: the percentage of rows less than or equal to the current row. - It returns a value larger than 0 and at most 1.\u00a0- Tied values are given the same cumulative distribution value. -&gt; Practice Link : https://www.datacamp.com/datalab/w/9af76c93-a717-4fab-afaa-15a719d1fe92/edit"},{"location":"Notes/Data%20Engineering/Tools/Flink/1.%20StateFul%20Stream%20Processing/","title":"1. StateFul Stream Processing","text":"<ul> <li>Stateful stream processing<ul> <li>Lowest level abstraction simply offers stateful and timely stream processing.It is embedded into the DataStream API via the Process Function. It allows users to freely process events from one or more streams, and provides consistent, fault tolerant state.</li> </ul> </li> <li>DataStream(bounded/unbounded streams) / DataSet(bounded data sets) API<ul> <li>These fluent APIs offer the common building blocks for data processing, like various forms of user-specified transformations, joins, aggregations, windows, state, etc.</li> </ul> </li> <li>Table API<ul> <li>The Table API follows the (extended) relational model: Tables have a schema attached (similar to tables in relational databases) and the API offers comparable operations, such as select, project, join, group-by, aggregate, etc. Table API programs declaratively define what logical operation should be done rather than specifying exactly how the code for the operation looks.</li> </ul> </li> <li>SQL<ul> <li>This abstraction is similar to the Table API both in semantics and expressiveness, but represents programs as SQL query expressions. The SQL abstraction closely interacts with the Table API</li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Tools/Flink/1.%20StateFul%20Stream%20Processing/#stateful-stream-processing","title":"Stateful Stream Processing","text":"<ul> <li>Keyed State : Acts like an embedded key/value store.The state is partitioned and distributed strictly together with the streams that are read by the stateful operators.Aligning the keys of streams and state makes sure that all state updates are local operations, guaranteeing consistency without transaction overhead. This alignment also allows Flink to redistribute the state and adjust the stream partitioning transparently.Keyed State is further organized into so-called Key Groups. Key Groups are the atomic unit by which Flink can redistribute Keyed State; there are exactly as many Key Groups as the defined maximum parallelism. During execution each parallel instance of a keyed operator works with the keys for one or more Key Groups.<ul> <li></li> </ul> </li> <li>State Persistence : Flink implements fault tolerance using a combination of stream replay and checkpointing. The fault tolerance mechanism continuously draws snapshots of the distributed streaming data flow. <ul> <li>Checkpoint/Snapshot : A checkpoint marks a specific point in each of the input streams along with the corresponding state for each of the operators.By setting the checkpoint interval we can decide the trade-off between overhead of fault tolerance and recovery time(the number of records that need to be replayed). Checkpointing can be done asynchronously.<ul> <li>Barriers<ul> <li>These barriers are injected into the data stream and flow with the records as part of the data stream.The barriers then flow downstream. When an intermediate operator has received a barrier for snapshot n from all of its input streams, it emits a barrier for snapshot n into all of its outgoing streams. Once a sink operator (the end of a streaming DAG) has received the barrier n from all of its input streams, it acknowledges that snapshot n to the checkpoint coordinator. After all sinks have acknowledged a snapshot, it is considered completed.</li> <li></li> <li>Barrier alignment for operator with multiple input stream<ul> <li>As soon as the operator receives snapshot barrier n from an incoming stream, it cannot process any further records from that stream until it has received the barrier n from the other inputs as well. Otherwise, it would mix records that belong to snapshot n and with records that belong to snapshot n+1.</li> <li>Once the last stream has received barrier n, the operator emits all pending outgoing records, and then emits snapshot n barriers itself.</li> <li>It snapshots the state and resumes processing records from all input streams, processing records from the input buffers before processing the records from the streams.</li> <li>Finally, the operator writes the state asynchronously to the state backend.</li> <li></li> <li>Recovery : Upon a failure, Flink selects the latest completed checkpoint k. The system then re-deploys the entire distributed dataflow, and gives each operator the state that was snapshotted as part of checkpoint k. The sources are set to start reading the stream from position Sk</li> </ul> </li> <li>Unalogned Checkpointing : Especially suited for applications with at least one slow moving data path<ul> <li>The operator reacts on the first barrier that is stored in its input buffers.</li> <li>It immediately forwards the barrier to the downstream operator by adding it to the end of the output buffers.</li> <li>The operator marks all overtaken records to be stored asynchronously and creates a snapshot of its own state.</li> <li></li> <li>Recovery : Operators first recover the in-flight data before starting processing any data from upstream operators in unaligned checkpointing. Aside from that, it performs the same steps as during recovery of aligned checkpoints.</li> </ul> </li> <li>Snapshotting operator state : Operators snapshot their state at the point in time when they have received all snapshot barriers from their input streams, and before emitting the barriers to their output streams. At that point, all updates to the state from records before the barriers have been made, and no updates that depend on records from after the barriers have been applied. The resulting snapshot now contains:<ul> <li>For each parallel stream data source, the offset/position in the stream when the snapshot was started</li> <li>For each operator, a pointer to the state that was stored as part of the snapshot</li> <li></li> </ul> </li> <li>State Backends<ul> <li>The exact data structures in which the key/values indexes are stored depends on the chosen state backend. One state backend stores data in an in-memory hash map, another state backend uses RocksDB as the key/value store. In addition to defining the data structure that holds the state, the state backends also implement the logic to take a point-in-time snapshot of the key/value state and store that snapshot as part of a checkpoint</li> <li></li> </ul> </li> </ul> </li> <li>Savepoints<ul> <li>Savepoints are manually triggered checkpoints, which take a snapshot of the program and write it out to a state backend. They rely on the regular checkpointing mechanism for this.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Data%20Engineering/Tools/Flink/2.%20Timely%20Stream%20Processing/","title":"2. Timely Stream Processing","text":"<p>When referring to time in a streaming program (for example to define windows), one can refer to different notions of time: * Processing time : Processing time refers to the system time of the machine that is executing the respective operation. When a streaming program runs on processing time, all time-based operations (like time windows) will use the system clock of the machines that run the respective operator. * Event Time : Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records. The progress of time depends on the data, not on any wall clocks. In a perfect world, event time processing would yield completely consistent and deterministic results, regardless of when events arrive, or their ordering. However, unless the events are known to arrive in-order (by timestamp), event time processing incurs some latency while waiting for out-of-order events.      *  Watermarks         * A stream processor that supports event time needs a way to measure the progress of event time.The mechanism in Flink to measure progress in event time is watermarks. Watermarks flow as part of the data stream and carry a timestamp t. A Watermark(t) declares that event time has reached time t in that stream, meaning that there should be no more elements from the stream with a timestamp t\u2019 &lt;= t (i.e. events with timestamps older or equal to the watermark).         *          *          * </p>"},{"location":"Notes/Data%20Engineering/Tools/Flink/3.%20Flink%20Cluster%20Architecture/","title":"3. Flink Cluster Architecture","text":"<p>Anatomy of a Flink Cluster  The Flink runtime consists of two types of processes: a JobManager and one or more TaskManagers.The Client is not part of the runtime and program execution, but is used to prepare and send a dataflow to the JobManager. After that, the client can disconnect (detached mode), or stay connected to receive progress reports (attached mode).  * JobManager :  The JobManager has a number of responsibilities related to coordinating the distributed execution of Flink Applications: it decides when to schedule the next task (or set of tasks), reacts to finished tasks or execution failures, coordinates checkpoints, and coordinates recovery on failures, among others. This process consists of three different components:     * ResourceManager : The ResourceManager is responsible for resource de-/allocation and provisioning in a Flink cluster. Flink implements multiple ResourceManagers for different environments and resource providers such as YARN, Kubernetes and standalone deployments.     * Dispatcher : The Dispatcher provides a REST interface to submit Flink applications for execution and starts a new JobMaster for each submitted job. It also runs the Flink WebUI to provide information about job executions.     * JobMaster : A JobMaster is responsible for managing the execution of a single JobGraph. Multiple jobs can run simultaneously in a Flink cluster, each having its own JobMaster. * TaskManager : The TaskManagers (also called workers) execute the tasks of a dataflow, and buffer and exchange the data streams. The smallest unit of resource scheduling in a TaskManager is a task slot. The number of task slots in a TaskManager indicates the number of concurrent processing tasks. Note that multiple operators may execute in a task slot. Each worker (TaskManager) is a JVM process, and may execute one or more subtasks in separate threads.     * Tasks and Operator Chains         * For distributed execution, Flink chains operator subtasks together into tasks. Each task is executed by one thread. Chaining operators together into tasks is a useful optimization: it reduces the overhead of thread-to-thread handover and buffering, and increases overall throughput while decreasing latency.         *      * Task Slots and Resources         * Each task slot represents a fixed subset of resources of the TaskManager. A TaskManager with three slots, for example, will dedicate 1/3 of its managed memory to each slot. Note that no CPU isolation happens here; currently slots only separate the managed memory of tasks. Tasks in the same JVM share TCP connections (via multiplexing) and heartbeat messages.Flink allows subtasks to share slots even if they are subtasks of different tasks, so long as they are from the same job.         *  * Flink application execution     * A Flink Application is any user program that spawns one or multiple Flink jobs from its <code>main()</code> method. The execution of these jobs can happen in a local JVM (<code>LocalEnvironment</code>) or on a remote setup of clusters with multiple machines (<code>RemoteEnvironment</code>).The jobs of a Flink Application can either be submitted to a long-running Flink Session Cluster, or a Flink Application Cluster         * Flink application cluster : Dedicated Flink cluster that only executes jobs from one Flink Application and where the <code>main()</code> method runs on the cluster rather than the client. The job submission is a one-step process: you don\u2019t need to start a Flink cluster first and then submit a job to the existing cluster session; instead, you package your application logic and dependencies into a executable job JAR and the cluster entrypoint (<code>ApplicationClusterEntryPoint</code>) is responsible for calling the <code>main()</code> method to extract the JobGraph.         * Flink session cluster : The client connects to a pre-existing, long-running cluster that can accept multiple job submissions. Even after all jobs are finished, the cluster (and the JobManager) will keep running until the session is manually stopped.</p>"},{"location":"Notes/Data%20Engineering/Tools/Flink/4.%20Data%20Transformations/","title":"4. Data Transformations","text":"<ul> <li>Stateless Transformation<ul> <li>map() : A function for performing one to one mapping.</li> <li>flapmap() : A <code>MapFunction</code> is suitable only when performing a one-to-one transformation: for each and every stream element coming in, <code>map()</code> will emit one transformed element. Otherwise, you will want to use <code>flatmap</code></li> <li>keyBy( ) : Partition a stream around one of its attributes, so that all events with the same value of that attribute are grouped together.</li> <li></li> </ul> </li> <li>Statefull Transformation : Features of state manged by Flink<ul> <li>local: Flink state is kept local to the machine that processes it, and can be accessed at memory speed</li> <li>durable: Flink state is fault-tolerant, i.e., it is automatically checkpointed at regular intervals, and is restored upon failure</li> <li>vertically scalable: Flink state can be kept in embedded RocksDB instances that scale by adding more local disk</li> <li>horizontally scalable: Flink state is redistributed as your cluster grows and shrinks</li> <li>queryable: Flink state can be queried externally via the Queryable State API.</li> </ul> </li> <li>Rich function </li> <li>Connected Stream</li> <li></li> </ul>"},{"location":"Notes/Data%20Engineering/Tools/Flink/Flink%20101/","title":"Flink 101","text":"<p>What it does :  Stateful Computations over Data Streams How it does : </p> <p>![[Screenshot from 2024-05-14 09-06-47.png]]</p> <p>Stream processing To analyze data, you can either organize your processing around\u00a0<code>bounded</code>\u00a0or\u00a0<code>unbounded</code>\u00a0streams, - Bounded Stream : It is the paradigm at work when you process a bounded data stream. In this mode of operation you can choose to ingest the entire dataset before producing any results, which means that it is possible, for example, to sort the data, compute global statistics, or produce a final report that summarizes all of the input. - Unbounded Stream : It\u00a0involves unbounded data streams. Conceptually, at least, the input may never end, and so you are forced to continuously process the data as it arrives. In Flink, applications are composed of\u00a0streaming dataflows\u00a0that may be transformed by user-defined\u00a0operators. These dataflows form directed graphs that start with one or more\u00a0sources, and end in one or more\u00a0sinks. - ![[program_dataflow.svg]]</p>"},{"location":"Notes/Data%20Engineering/Tools/Spark/101/","title":"101","text":"<p>High level APIs - Spark SQL - Pandas API on spark - MLib - GraphX -  Structured streaming</p> <p>-&gt; Core Computational Concepts - MapReduce - RDD -&gt; Orchestration and deployment - Internal components - Cloud Specific components - Tuning Oprimizations - Salting</p> <p>Interview Topics</p>"},{"location":"Notes/Data%20Engineering/Tools/Spark/Core/","title":"Core","text":"<p>-&gt; Map reduce  - Programming model and associated implementation of processing of large datasets</p> <p> The Map invocations are distributed across multiple machines by automatically partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function. -&gt; Steps - The MapReduce library in the user program first splits the input files into M pieces of typically 16 megabytes to 64 megabytes (MB) per piece.  It then starts up many copies of the program on a cluster of machines. One of the copies of the program is special \u2013 the master. The rest are workers that are assigned work by the master. - A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined Map function. The intermediate key/value pairs produced by the Map function are buffered in memory. - Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function.The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers. - When a reduce worker is notified by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in memory, an external sort is used. - The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user\u2019s Reduce function. The output of the Reduce function is appended to a final output file for this reduce partition - When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.</p> <p>-&gt; Master Data Structure : For each mapReduce task following data are stores - State( idle, in-progress, completed ) - Identity of worker machine - Map Tasks : Location and size of R intermediate file region produced by map task, these are send back to reduce workers</p> <p>-&gt;  Fault tolerance - Worker failure - The master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. All the task in them are set to idle state and becomes eligible for scheduling.  - Master Failure - Periodic checkpoints are written of the master data structure, On master failure a new master can start from last checkpointed state. - </p>"},{"location":"Notes/Data_Structure_And_Algorithm/Arrays/","title":"Arrays","text":"<p>Arrays</p> <p>SDE Sheet</p> <ul> <li> <p>Set Matrix Zero - Leetcode - Tutorial - Ask for time and space complexity, intelligent soln</p> </li> <li> <p>Pascal's Triangle - LeetCode Simulation</p> </li> <li> <p>Next Permutation - LeetCode ** Good Question - Simulation</p> </li> <li> <p>Maximum Subarray - LeetCode Kandane\u2019s Problem ** , Sim : Best Time to Buy and Sell Stock - LeetCode</p> </li> <li> <p>Merge Intervals - LeetCode Good strategy</p> </li> <li> <p>Find the Duplicate Number - LeetCode Pigeonhole principle</p> </li> <li> <p>Count inversions in an array Using Merge Sort, Understand how complexity changes from O(N^2) to O(NLogN)</p> </li> <li> <p>Pow(x, n) - LeetCode Binary Exponentiation</p> </li> <li> <p>Majority Element - LeetCode Moore\u2019s Voting Algorithm **</p> </li> <li> <p>Majority Element II - LeetCode\u00a0 Extended Boyer Moore\u2019s Voting Algorithm **</p> </li> <li> <p>Unique Paths - LeetCode Recursive Soln | DP Soln</p> </li> <li> <p>Reverse Pairs - LeetCode /\u00a0 Count of Smaller Numbers After Self - LeetCode\u00a0 **</p> </li> <li> <p>3Sum - LeetCode / 4Sum - LeetCode</p> </li> <li> <p>Longest Consecutive Sequence - LeetCode **</p> </li> <li> <p>Largest subarray with 0 sum | Practice | GeeksforGeeks ** / Subarray Sum Equals K - LeetCode</p> </li> <li> <p>https://www.codingninjas.com/codestudio/problems/1115652?topList=striver-sde-sheet-problems&amp;utm_source=striver&amp;utm_medium=website</p> </li> <li> <p>https://takeuforward.org/data-structure/count-the-number-of-subarrays-with-given-xor-k/ - https://www.codingninjas.com/codestudio/problems/1115652?topList=striver-sde-sheet-problems&amp;utm_source=striver&amp;utm_medium=website</p> </li> <li> <p>Good Problems</p> <ul> <li>https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/</li> <li>https://leetcode.com/problems/jump-game-ii/description/?envType=study-plan-v2&amp;envId=top-interview-150</li> <li>https://leetcode.com/problems/jump-game/description/?envType=study-plan-v2&amp;envId=top-interview-150</li> <li>https://leetcode.com/problems/product-of-array-except-self/?envType=study-plan-v2&amp;envId=top-interview-150</li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Arrays/#pattern","title":"Pattern","text":"<ul> <li>One Pass : https://en.wikipedia.org/wiki/One-pass_algorithm</li> <li>Two pointer</li> <li>Sliding Window</li> <li> <p>Prefix sum</p> </li> <li> <p>Proof by contradiction</p> </li> <li></li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Backtracking/","title":"Backtracking","text":"<p>Backtracking is a general algorithm for finding solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a valid solution. Backtracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute-force enumeration of all complete candidates, since it can eliminate many candidates with a single test. Backtracking is an important tool for solving constraint satisfaction problems,2 such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient technique for parsing,3 for the knapsack problem and other combinatorial optimization problems. Backtracking depends on user-given \"black box procedures\" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm\u00a0\u2013 although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time.</p> <p>The backtracking algorithm enumerates a set of partial candidates that, in principle, could be completed in various ways to give all the possible solutions to the given problem. The completion is done incrementally, by a sequence of candidate extension steps.</p> <p>Conceptually, the partial candidates are represented as the nodes of a tree structure, the potential search tree. Each partial candidate is the parent of the candidates that differ from it by a single extension step; the leaves of the tree are the partial candidates that cannot be extended any further.</p> <p>The backtracking algorithm traverses this search tree recursively, from the root down, in depth-first order. At each node c, the algorithm checks whether c can be completed to a valid solution. If it cannot, the whole sub-tree rooted at c is skipped (pruned). Otherwise, the algorithm (1) checks whether c itself is a valid solution, and if so reports it to the user; and (2) recursively enumerates all sub-trees of c. The two tests and the children of each node are defined by user-given procedures.</p> <p>Therefore, the actual search tree that is traversed by the algorithm is only a part of the potential tree. The total cost of the algorithm is the number of nodes of the actual tree times the cost of obtaining and processing each node. This fact should be considered when choosing the potential search tree and implementing the pruning test.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Backtracking/#pseudocode","title":"Pseudocode","text":"<p>In order to apply backtracking to a specific class of problems, one must provide the data P for the particular instance of the problem that is to be solved, and six procedural parameters, root, reject, accept, first, next, and output. These procedures should take the instance data P as a parameter and should do the following:</p> <ol> <li>root(P): return the partial candidate at the root of the search tree.</li> <li>reject(P,c): return true only if the partial candidate c is not worth completing.</li> <li>accept(P,c): return true if c is a solution of P, and false otherwise.</li> <li>first(P,c): generate the first extension of candidate c.</li> <li>next(P,s): generate the next alternative extension of a candidate, after the extension s.</li> <li>output(P,c): use the solution c of P, as appropriate to the application.</li> </ol> <p>The backtracking algorithm reduces the problem to the call backtrack(root(P)), where backtrack is the following recursive procedure:</p> <ul> <li>procedure backtrack(c) is<ul> <li>if reject(P, c) then return</li> <li>if accept(P, c) then output(P, c)</li> <li>s \u2190 first(P, c)</li> <li>while s \u2260 NULL do<ul> <li>backtrack(s)</li> <li>s \u2190 next(P, s)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Backtracking/#problems","title":"Problems","text":"<ul> <li>N Queen Problem</li> <li>Sudoku Solver</li> <li>Rat in a Maze Problem good question</li> <li>Map Coloring Problem</li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Binary%20Tree/","title":"Binary Tree","text":"<p>d A\u00a0Binary Tree Data Structure\u00a0is a hierarchical data structure in which each node has at most two children, referred to as the left child and the right child. It is commonly used in computer science for efficient storage and retrieval of data, with various operations such as insertion, deletion, and traversal.The topmost node in a binary tree is called the root, and the bottom-most nodes are called leaves.</p> <p>Representation of Binary Tree** Each node in the tree contains the following: -  Data -  Pointer to the left child -  Pointer to the right child</p> <p>Basic Operations On Binary Tree: - Inserting an element. - Removing an element. - Searching for an element. - Deletion for an element. - Traversing an element.</p> <p>Auxiliary Operations On Binary Tree: -  Finding the height of the tree -  Find the level of the tree -  Finding the size of the entire tree.</p> <p>Binary Tree Traversal * Depth-First Search (DFS) Algorithms     * Preorder Traversal (current-left-right)     * Inorder Traversal (left-current-right)     * Postorder Traversal (left-right-current) * Breadth-First Search (BFS) Algorithms     * Level Order Traversal</p> <p>Properties if Binary Tree *  The maximum number of nodes at level \u2018l\u2019 of a binary tree is 2^l *  The\u00a0Maximum number of nodes in a binary tree of height \u2018h\u2019 is 2^(h+1) - 1 , where height of root node is zero *  \u00a0In a Binary Tree with N nodes, the minimum possible height or the minimum number of levels is Log2(N+1) *  A Binary Tree with L leaves has at least | Log2 L |+ 1 \u00a0 levels *  Nodes that do not have any child nodes are called leaf nodes * </p>"},{"location":"Notes/Data_Structure_And_Algorithm/Binary%20Tree/#very-important-to-understand-is-the-order-of-function-calls-stack-generated-during-the-tree-traversal","title":"Very important to understand is the order of function calls (Stack generated during the tree traversal).","text":"<ul> <li> <p>Tree traversal</p> <ul> <li>Pre Order traversal 1 -&gt; 2 -&gt; 4 -&gt; 5 -&gt; 3 -&gt; 6.<ul> <li> </li> </ul> </li> <li>In Order 4 -&gt; 2 -&gt; 5 -&gt; 1 -&gt; 3 -&gt; 6.<ul> <li></li> </ul> </li> <li>Post Order Traversal 4 -&gt; 5 -&gt; 2 -&gt; 6 -&gt; 3 -&gt; 1.<ul> <li></li> </ul> </li> </ul> </li> <li> <p>Array Implementation of Binary tree Link</p> </li> </ul> <pre><code> Case 1:**\u00a0(0 -&gt; n-1)\u00a0\n\n    if (say)father=p;\u00a0\n    then left_son=(2*p)+1;\u00a0\n    and right_son=(2*p)+2;\n\n       A(0)    \n     /   \\\n    B(1)  C(2)  \n  /   \\      \\\n D(3)  E(4)   F(6)\n\n\nCase 2:**\u00a0(1 \u2014&gt; n)\n\n    if (say)father=p;\u00a0\n    then left_son=(2*p);\u00a0\n    and right_son=(2*p)+1;\n\n      A(1)    \n     /   \\\n    B(2)  C(3)  \n  /   \\      \\\n D(4)  E(5)   F(7)\n</code></pre>"},{"location":"Notes/Data_Structure_And_Algorithm/Binary_Search/","title":"Binary Search","text":"<p>Binary search, also known as half-interval search,1 logarithmic search,2,subsection\"Binary_search\"-2) or binary chop,3 is a search algorithm that finds the position of a target value within a sorted array.45 Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.Binary search runs in logarithmic time in the worst case, making O(log n) comparisons, where n is the number of elements in the array.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Binary_Search/#procedure","title":"Procedure","text":"<p>Given an array A {\\displaystyle A}  of n {\\displaystyle n}  elements with values or records A 0 , A 1 , A 2 , \u2026 , A n \u2212 1 {\\displaystyle A_{0},A_{1},A_{2},\\ldots ,A_{n-1}}  sorted such that A 0 \u2264 A 1 \u2264 A 2 \u2264 \u22ef \u2264 A n \u2212 1 {\\displaystyle A_{0}\\leq A_{1}\\leq A_{2}\\leq \\cdots \\leq A_{n-1}}  , and target value T {\\displaystyle T}  , the following subroutine uses binary search to find the index of T {\\displaystyle T}  in A {\\displaystyle A}  .[7],subsection\"Algorithm_B\"-8)</p> <ol> <li>Set L {\\displaystyle L}  to 0 {\\displaystyle 0}  and R {\\displaystyle R}  to n \u2212 1 {\\displaystyle n-1}  .</li> <li>If L &gt; R {\\displaystyle L&gt;R}  , the search terminates as unsuccessful.</li> <li>Set m {\\displaystyle m}  (the position of the middle element) to the floor of L + R 2 {\\displaystyle {\\frac {L+R}{2}}}  , which is the greatest integer less than or equal to L + R 2 {\\displaystyle {\\frac {L+R}{2}}}  .</li> <li>If A m &lt; T {\\displaystyle A_{m}&lt;T}  , set L {\\displaystyle L}  to m + 1 {\\displaystyle m+1}  and go to step 2.</li> <li>If A m &gt; T {\\displaystyle A_{m}&gt;T}  , set R {\\displaystyle R}  to m \u2212 1 {\\displaystyle m-1}  and go to step 2.</li> <li>Now A m = T {\\displaystyle A_{m}=T}  , the search is done; return m {\\displaystyle m}  .</li> </ol> <p>This iterative procedure keeps track of the search boundaries with the two variables L {\\displaystyle L}  and R {\\displaystyle R}  . The procedure may be expressed in pseudocode as follows, where the variable names and types remain the same as above, <code>floor</code> is the floor function, and <code>unsuccessful</code> refers to a specific value that conveys the failure of the search.[7],subsection\"Algorithm_B\"-8)</p> <ul> <li>function binary_search(A, n, T) is<ul> <li>L\u00a0:= 0</li> <li>R\u00a0:= n \u2212 1</li> <li>while L \u2264 R do<ul> <li>m\u00a0:= floor((L + R) / 2)</li> <li>if A[m] &lt; T then<ul> <li>L\u00a0:= m + 1</li> </ul> </li> <li>else if A[m] &gt; T then<ul> <li>R\u00a0:= m \u2212 1</li> </ul> </li> <li>else:<ul> <li>return m</li> </ul> </li> </ul> </li> <li>return unsuccessful</li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Binary_Search/#problems","title":"Problems","text":"<ul> <li>Nth Root Of M Using Binary Search</li> <li>2D Matrix<ul> <li>Find Median</li> <li>Search a 2D Matrix - Search a 2D Matrix II</li> </ul> </li> <li></li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Binary_Search/#refer-httpswwwgeeksforgeeksorgdivide-and-conquer-for-binary-search-problems","title":"Refer https://www.geeksforgeeks.org/divide-and-conquer/ For Binary search problems","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Binary_Search_Tree/","title":"Binary Search Tree","text":"<p>A\u00a0*Binary Search Tree\u00a0is a data structure used in computer science for organizing and storing data in a sorted manner. Each node in a\u00a0Binary Search Tree\u00a0has at most two children, a\u00a0left\u00a0child and a\u00a0right\u00a0child, with the\u00a0left\u00a0child containing values less than the parent node and the\u00a0right\u00a0child containing values greater than the parent node. This hierarchical structure allows for efficient\u00a0searching,\u00a0insertion, and\u00a0deletion*\u00a0operations on the data stored in the tree.</p> <p> * Properties     * The left subtree of a node contains only nodes with keys lesser than the node\u2019s key.     * The right subtree of a node contains only nodes with keys greater than the node\u2019s key.     * The left and right subtree each must also be a binary search tree * Traversal     * InOrder traversal if an BST returns the values in a sorted manner</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Common_Techniques/","title":"Common Techniques","text":"<ul> <li>Generating permutations of an array : Recursive approach to include or reject an element in every step<ul> <li>Time complexity : O(2^N) </li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/DIvide_and_Conquer/","title":"DIvide and Conquer","text":"<p>A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem. The name divide and conquer is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search.The name decrease and conquer has been proposed instead for the single-subproblem class. On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion. In this case, whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; avoids half the function calls in some algorithms on binary trees. For some problems, the branched recursion may end up evaluating the same sub-problem many times over. In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique is commonly known as memoization.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/DIvide_and_Conquer/#problems","title":"Problems","text":"<ul> <li>Find Median<ul> <li>Median of Two Sorted Arrays of Same Size expected TC :  O(log(n))</li> <li>Median of Two Sorted Arrays of Different Size - LeetCode GFG</li> </ul> </li> <li>Greatest Common Divisor</li> <li>Multiply 2 n digit number - Karatsuba LeetcodeGFG</li> <li>Matrix Multiplication - Strassen GFG</li> <li>Tower Of Hanoi</li> <li>Search a 2D Matrix II</li> <li>Peak Element<ul> <li>In 1D Array</li> <li>In 2D Array</li> </ul> </li> <li>Find Array Given Subset Sums</li> <li>Closest Point<ul> <li>K Closest Points to Origin </li> <li>GFG 1 2</li> <li></li> </ul> </li> <li>The Skyline Problem Leetcode GFG</li> <li>Convex Hull LeetCodeGFG</li> <li></li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Dynamic_Programming/","title":"Dynamic Programming","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Dynamic_Programming/#list","title":"List","text":"<ul> <li>Palindrome Partitioning Todo</li> <li>Longest Valid Parentheses</li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Graph/","title":"Graph","text":"<p>Graph is a non-linear data structure consisting of vertices and edges. The vertices are sometimes also referred to as nodes and the edges are lines or arcs that connect any two nodes in the graph. More formally a\u00a0Graph\u00a0is composed of a set of vertices(\u00a0*V\u00a0) and a set of edges(\u00a0E\u00a0). The graph is denoted by\u00a0G(V, E).*</p> <p>Type of graph *  Null Graph : if there are no edges in the graph. *  Trivial Graph : Graph having only a single vertex *  Undirected Graph : edges do not have any direction * Directed Graph : \u00a0edge has direction *  Connected Graph : one node we can visit any other node in the graph *  Disconnected Graph : at least one node is not reachable from a node *  Regular Graph : degree of every vertex is equal to K is called K regular graph. *  Complete Graph : graph in which from each node there is an edge to each other node. *  Cycle Graph : graph in which the graph is a cycle in itself, the degree of each vertex is 2. *  Cyclic Graph : graph containing at least one cycle is known as a Cyclic graph. *  Directed Acyclic Graph : Directed Graph that does not contain any cycle. *  Bipartite Graph : graph in which vertex can be divided into two sets such that vertex in each set does not contain any edge between them. * Labeled/Weighted Graph : edges are already specified with suitable weight is known as a weighted graph, can be further classified as directed weighted graphs and undirected weighted graphs. * Simple Graph: A simple graph is a graph that does not contain more than one edge between the pair of vertices. * Multi Graph : Any graph which contains some parallel edges but doesn\u2019t contain any self-loop is called a multigraph     * Parallel Edges:\u00a0If two vertices are connected with more than one edge then such edges are called parallel edges that are many routes but one destination.     - Loop:\u00a0An edge of a graph that starts from a vertex and ends at the same vertex is called a loop or a self-loop. - Pseudo Graph : A graph G with a self-loop and some multiple edges is called a pseudo graph. -  Subgraph : A graph G1 = (V1, E1) is called a subgraph of a graph G(V, E) if V1(G) is a subset of V(G) and E1(G) is a subset of E(G) such that each edge of G1 has same end vertices as in G.     - Vertex disjoint subgraph:\u00a0Any two graph G1 = (V1, E1) and G2 = (V2, E2) are said to be vertex disjoint of a graph G = (V, E) if V1(G1) intersection V2(G2) = null.     - Edge disjoint subgraph:\u00a0A subgraph is said to be edge-disjoint if E1(G1) intersection E2(G2) = null.     - Spanning Subgraph : A spanning subgraph is a subgraph that contains all the vertices of the original graph G that is G'(V\u2019,E\u2019) is spanning if V\u2019=V and E\u2019 is a subset of E.</p> <p>Representation of Graphs  -  Adjacency Matrix  -  Adjacency List</p> Action Adjacency Matrix Adjacency List Adding Edge O(1) O(1) Removing an edge O(1) O(N) Initializing O(N*N) O(N) - Graph Traversal - Breadth First Search : It explores all the vertices in a graph at the current depth before moving on to the vertices at the next depth level. It starts at a specified vertex and visits all its neighbours before moving on to the next level of neighbours. Applications are - Shortest Path Finding:\u00a0BFS can be used to find the shortest path between two nodes in an unweighted graph. By keeping track of the parent of each node during the traversal, the shortest path can be reconstructed. - Cycle Detection:\u00a0BFS can be used to detect cycles in a graph. If a node is visited twice during the traversal, it indicates the presence of a cycle. - Connected Components:\u00a0BFS can be used to identify connected components in a graph. Each connected component is a set of nodes that can be reached from each other. - Topological Sorting:\u00a0BFS can be used to perform topological sorting on a directed acyclic graph (DAG). Topological sorting arranges the nodes in a linear order such that for any edge (u, v), u appears before v in the order. - Level Order Traversal of Binary Trees:\u00a0BFS can be used to perform a level order traversal of a binary tree. This traversal visits all nodes at the same level before moving to the next level. - Network Routing : BFS can be used to find the shortest path between two nodes in a network, making it useful for routing data packets in network protocols. <p>Patterns - Keeping track of traversed nodes</p> <ul> <li>Algorithms<ul> <li></li> <li>Depth First search : Given a graph, how can we find all of its vertices, and how can we find all paths between two vertices.<ul> <li>Either stack or recursion can be used to implement DFS</li> <li>Complexity analysis<ul> <li> <ul> <li>Time Complexity:\u00a0O(V+E)O(V+E). Here,\u00a0VV\u00a0represents the number of vertices, and\u00a0EE\u00a0represents the number of edges. We need to check every vertex and traverse through every edge in the graph.</li> </ul> </li> <li> <ul> <li>Space Complexity:\u00a0O(V2)O(V2). Either the manually created stack or the recursive call stack can store up to\u00a0V\u22c5VV\u22c5V\u00a0vertices in the worst case when each vertex has edges connecting to all other vertices because we push vertices to the stack before checking whether they have been visited. </li> </ul> </li> </ul> </li> <li>Algorithm : A standard DFS implementation puts each vertex of the graph into one of two categories:<ul> <li>Visited</li> <li>Not Visisted</li> <li>The purpose of the algorithm is to mark each vertex as visited while avoiding cycles. The DFS algorithm works as follows<ul> <li>Start by putting any one of the graph's vertices on top of a stack.</li> <li>Take the top item of the stack and add it to the visited list.</li> <li>Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the top of the stack.</li> <li>Keep repeating steps 2 and 3 until the stack is empty.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/","title":"Greedy Algorithms","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/#optimal-substructure","title":"Optimal Substructure","text":"<p>A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/#identify-a-problem-to-be-solved-by-greedy-approach","title":"Identify a problem to be solved by greedy approach","text":"<p>Most problems for which they work will have two properties * <code>Greedy choice property</code> : When we are considering which choice to make, we make the choice that looks best in the current problem, without considering results from subproblems. We can assemble a globally optimal solution by making locally optimal (greedy) choices. * <code>Optimal sunstructure</code> : A problem exhibits this property if an optimal solution to the problem contains within it optimal solutions to subproblems.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/#steps-to-design-a-greedy-solution","title":"Steps to design a greedy solution","text":"<ul> <li>Cast the optimisation problem as one in which we make a choice and are left with one subproblem to solve.</li> <li>Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.</li> <li>Demonstrate optimal substructure by showing that, having made the greedy choice, what remains is a subproblem with the property that if we combine an optimal solution to the subproblem with the greedy choice we have made, we arrive at an optimal solution to the original problem.</li> </ul> <p>For interview it is important to remember the patterns, as it hard to prove that the problem can be solved by greedy approach</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/#different-types-of-greedy-algorithm-programiz-list","title":"Different Types of Greedy Algorithm Programiz list","text":"<ul> <li>Selection Sort</li> <li>Knapsack Problem</li> <li>Minimum Spanning Tree</li> <li>Single-Source Shortest Path Problem</li> <li>Job Scheduling Problem</li> <li>Prim's Minimal Spanning Tree Algorithm</li> <li>Kruskal's Minimal Spanning Tree Algorithm</li> <li>Dijkstra's Minimal Spanning Tree Algorithm</li> <li>Huffman Coding</li> <li>Ford-Fulkerson Algorithm</li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/#problem-list-vague-cluster","title":"Problem List (Vague cluster)","text":"<ul> <li>Standard Greedy Algorithms <ul> <li>Intervals<ul> <li>Minimum Number of Arrows to Burst Balloons/  Minimum Platforms) Activity Selection Problem  /  Meeting Room  / </li> </ul> </li> <li>Greedy Algorithm for Egyptian Fraction</li> <li>[Job Sequencing Problem]</li> <li>Fractional Knapsack Problem</li> <li>Find Minimum Number Of Coins</li> <li></li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Greedy_Algorithms/#-max-profit-and-number-of-jobs-gfg-codingninja","title":"- Max Profit and number of Jobs GFG / CodingNinja","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Heap_Priority_Queue/","title":"Heap Priority Queue","text":"<p>A heap is a specialized treewhich is essentially an almost complete (When a heap is a complete binary tree, it has a smallest possible height\u2014a heap with N nodes and a branches for each node always has log__a__N height.This property of Binary Heap makes them suitable to be stored in an array.)tree that satisfies the heap property: in a max heap, for any given node.</p> <p>A complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes in the last level are as far left as possible. It can have between 1 and 2h nodes at the last level h. A perfect tree is therefore always complete but a complete tree is not necessarily perfect. An alternative definition is a perfect tree whose rightmost leaves (perhaps all) have been removed. Some authors use the term complete to refer instead to a perfect binary tree as defined above, in which case they call this type of tree (with a possibly not filled last level) an almost complete binary tree or nearly complete binary tree. A complete binary tree can be efficiently represented using an array.</p> <p>The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as \"heaps\", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered.</p> <p>A common implementation of a heap is the binary heap, in which the tree is a binary tree </p>"},{"location":"Notes/Data_Structure_And_Algorithm/Heap_Priority_Queue/#refer-clrs-ch-heapsort","title":"Refer CLRS (Ch. Heapsort)","text":"<p>A binary heap is typically represented as an array.A priority queue is a data structure for maintaining a set S of elements, each with an associated value called a key.An array Arr that represents a heap is an object with two at- tributes: Arr.length, which (as usual) gives the number of elements in the array, and Arr.heap-size, which represents how many elements in the heap are stored within array A. That is, although Arr[1 ... Arr.length] may contain numbers, only the elements in Arr[1 ... Arr.heap-size], where 0 &lt;= A:heap-size &lt;= A:length, are valid elements of the heap.</p> <ul> <li>The root elem     ent will be at Arr[0].</li> <li> <p>Below table shows indexes of other nodes for the ith node, i.e., Arr[i]:  </p> <ul> <li>Parent(i) : Arr[ (i-1)/2 ]  Returns the parent node</li> <li>Left(i) : Arr[(2*i) + 1] Returns the left child node</li> <li>Right(i) : Arr[(2*i)+2]  Returns the right child node</li> </ul> </li> <li> <p>Heap operations (Max-Heap example)</p> <ul> <li>maxHeapify(Arr,i) : Mantain min/max heap property for a given array and an element - <code>O(Log n)</code><ul> <li>When it is called, MAX-HEAPIFY assumes that the binary trees rooted at LEFT[i] and RIGHT[i] are max-heaps, but that Arr[i] might be smaller than its children, thus violating the max-heap property. MAX-HEAPIFY lets the value at Arr[i] \u201c\ufb02oat down\u201d in the max-heap so that the subtree rooted at index i obeys the max-heap property.</li> </ul> </li> <li>buildMaxHeap() : For given element build a heap,Since the leaves are the nodes indexed by floor(n/2) , floor(n/2) + 1, floor(n/2) + 2, ... n, so each lement is a one element heap to begin with, we run the maxHeapify() in a bottom-up approach to build the heap. Refer CLRS(3rd Edition) 6.3 to find proof.</li> <li>extractMax() : Removes and returns the root - <code>O(Log n)</code></li> <li>inreaseKey(Arr,i,j) : Increase the key i to j. - <code>O(Log n)</code></li> <li>insertKey(Arr,i) : Insert a element with key i.</li> <li>getMax() :  Return the root node.</li> <li>deleteKey(Arr, i) : Deletes a key i. </li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Heap_Priority_Queue/#problems","title":"Problems","text":"<ul> <li>Heap Implementation</li> <li>Kth Largest Element in an Array<ul> <li>Unique Elements</li> <li>Repeated Elements</li> </ul> </li> <li>Top K Frequent Elements</li> <li>Merge k Sorted Lists</li> <li></li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Heuristics/","title":"Heuristics","text":"<p>Optimisation problem can have following * Optimal substructure     * Overlapping sub problem is present         * Dynamic programming     * No Overlapping of solution         * Greedy solution</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Intervals/","title":"Intervals","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Intervals/#reference","title":"Reference","text":"<ul> <li>https://www.techinterviewhandbook.org/algorithms/interval/</li> <li>https://www.geeksforgeeks.org/interval-tree/</li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Java/","title":"Java","text":"<p>Most commonly used data structure * Operator <pre><code>/*\nLogical\n    And : &amp;&amp;\n    OR  : ||\n*/\n</code></pre> * Type casting <pre><code>long k = (long)2147483647 \n</code></pre> * Array <pre><code>int[] arr = new int[10]; // value initialised with zero\n// Length of an array\nint arrLength = arr.length ; // A property rather than a method\n// Slicing a array\nint[] arr = {10, 20, 30, 40, 50};\nArrays.copyOfRange(arr, 1, 4); // returns {20, 30, 40}\n// Declaring a array of array and assigning value;\nint[][] op = new int[4][2];\nop[0] = new int[]{1,2};\n</code></pre> * List <pre><code>// Declaring a empty list\nList&lt;Integer&gt; intList = new ArrayList&lt;Integer&gt;();\n// Adding a element\nintList.add(69);\n// Declaring a list of list\nList&lt;List&lt;Integer&gt;&gt; levels = new ArrayList&lt;List&lt;Integer&gt;&gt;();\n</code></pre> * HashMap <pre><code>// Import\nimport java.util.HashMap;\n// declaring a empty HashMap\nHashMap&lt;String, Integer&gt; strIntHashMap = new HashMap&lt;String, Integer&gt;();\n// Adding an element\nstrIntHashMap.put(\"key1\",1);\nstrIntHashMap.put(\"key2\",2);\n// Getting an element\nstrIntHashMap.get(\"key1\");\n// Checking a key in the HashMap\nstrIntHashMap.containsKey(\"key1\") ; // returns True\n// Removeing a key\nstrIntHashMap.remove(\"key1\") ; // returns True\n// Iterate all elements\nIterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = map.entrySet().iterator(); \nwhile (iterator.hasNext()) { \n    Map.Entry&lt;String, String&gt; entry = iterator.next(); \n    String key = entry.getKey(); \n    String value = entry.getValue(); \n    System.out.println(\"Key=\" + key + \", Value=\" + value); \n}\n// \n</code></pre> * String <pre><code>// Decalring a string\nString sampleStr = \"boka\";\n// Getting a character at a index\nsampleStr.charAt(i);\n// initialising a string of size N with '0' character\nchar carr[] = new char[N];\nArrays.fill(carr, '0');\nretString = new String(carr);\n// replace a character/sub string from a string\nsampleStr.replace(\"bo\",\"da\");\n// spliting a string\nsampleStr.split(\"/\",0);\n// Equality sheck\nstr1.equals(str2);\n</code></pre> * Set <pre><code>// Declaring a Set\nSet&lt;Obj&gt; set = new HashSet&lt;Obj&gt; ();\n// adding an element\nset.add(element)\n// removing an element\nset.remove(element)\n// Checking whether a set contains an element\nset.contains(element)\n// Clearing a collection - delete all objects\nset.clear()\n// Size of the set\nset.size()\n</code></pre> * Common tricks <pre><code>// Character to ASCII \nint chASCII = 'a';\n// ASCII to int\nchar\u00a0c=(char)chASCII;\n// Integer to string\nint i = 1234;\nString str = Integer.toString(i);\n// String to Integer\nint\u00a0i = Integer.parseInt(\"200\");\n// List of string to string\nString str = String.join(\",\", strArr);\n// Switch case\n switch(expression){\u00a0\u00a0\u00a0\u00a0\n case\u00a0value1:\u00a0\u00a0\u00a0\u00a0\n \u00a0//code\u00a0to\u00a0be\u00a0executed;\u00a0\u00a0\u00a0\u00a0\n \u00a0break;\u00a0\u00a0//optional\u00a0\u00a0\n case\u00a0value2:\u00a0\u00a0\u00a0\u00a0\n \u00a0//code\u00a0to\u00a0be\u00a0executed;\u00a0\u00a0\u00a0\u00a0\n \u00a0break;\u00a0\u00a0//optional\u00a0\u00a0\n ......\u00a0\u00a0\u00a0\u00a0\n\n default:\u00a0\u00a0\u00a0\u00a0\u00a0\n \u00a0\u00a0code\u00a0to\u00a0be\u00a0executed\u00a0if\u00a0all\u00a0cases\u00a0are\u00a0not\u00a0matched;\u00a0\u00a0\n</code></pre></p> <pre><code>\n</code></pre> <p><pre><code>https://www.javatpoint.com/java-tuple\n</code></pre> * Java matrix <pre><code>// declare empty matrix\nint[][] matrix1 = new int[2][2];\nint matrix2[][] = new int[2][3];\n// Declare matrix with values\nString[][] matrix5 = { \n    { \"a\", \"lion\", \"meo\" }, \n    { \"jaguar\", \"hunt\" } \n};\n// getting # rows and # columns\nint[][] matrix = {{1,2},{3,4},{5,6}}\nint numRows = matrix.length;\nint numCOls = matrix[0].length;\n</code></pre> * General patterns     * Sorting an array of Java object based on one of its  attribute     <pre><code>import java.util.Arrays;\nimport java.util.Comparator;\n\nclass SortArrayOfArray{\n    public static void sortFunc( int[][] arrayOfArray ){\n        if ( arrayOfArray.length == 0 )\n            return ;\n        Arrays.sort( arrayOfArray , new Comparator&lt;int[]&gt;() {\n            public int compare(int[] arr1 , int[] arr2){\n                return arr1[0] - arr2[0];\n            }\n        });\n    }\n}\n</code></pre> * Stack <pre><code>// Declare a Stack\nStack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;();\nstack.push(elem);\nstack.pop(elem);\nstack.peek(); // Returns the top element\nstack.empty(); // Checks whether a stack is empty or not\n</code></pre></p>"},{"location":"Notes/Data_Structure_And_Algorithm/Linked_List/","title":"Linked List","text":"<p>General pattern and techniques</p> <p>Two/multiple pointer (Slow and fast)\u00a0</p> <ul> <li> <p>Reverse Linked List - LeetCode Without copying into a new linked List ( Iterative and recursive )</p> </li> <li> <p>Middle of the Linked List - LeetCode Naive approach is Good,(Tortoise-Hare-Approach Doesn\u2019t seems to be any more efficient )</p> </li> <li> <p>Merge Two Sorted Lists - LeetCode Use Inplace Method without using extra space</p> </li> <li> <p>Remove Nth Node From End of List - LeetCode Two pointer Method seems to save an extra O(N) Traversal</p> </li> <li> <p>Delete Node in a Linked List - LeetCode</p> </li> <li> <p>Intersection of Two Linked Lists - LeetCode</p> </li> <li> <p>Linked List Cycle - LeetCode Using O(1) Space and O(N) time. Linked List Cycle II - LeetCode</p> </li> <li> <p>Reverse Nodes in k-Group - LeetCode Good Question</p> </li> <li> <p>Palindrome Linked List - LeetCode </p> </li> <li> <p>Linked List Cycle II - LeetCode</p> </li> <li> <p>Flattening a Linked List | Practice | GeeksforGeeks</p> </li> <li> <p>Trapping Rain Water - LeetCode</p> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Paradigm/","title":"Paradigm","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Paradigm/#how-to-approach-a-problem-that-doesnt-fits-into-any-given-category","title":"How to approach a problem that doesn't fits into any given category","text":"<ul> <li>Simulate the process<ul> <li>Define how the event progresses </li> <li>Define the parameter on which the event progresses<ul> <li>E.g. : https://practice.geeksforgeeks.org/problems/minimum-platforms-1587115620/1# Here the event progresses on time.</li> </ul> </li> <li></li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/","title":"Recursion","text":"<p>Recursion (adjective: recursive) occurs when a thing is defined in terms of itself or of its type. In mathematics and computer science, a class of objects or methods exhibits recursive behavior when it can be defined by two properties:</p> <ul> <li>A simple base case (or cases) \u2014 a terminating scenario that does not use recursion to produce an answer</li> <li>A recursive step \u2014 a set of rules that reduces all successive cases toward the base case.</li> </ul> <p>Recursion reduces to one or more similar small problem till a base condition is met.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/#memory-allocation","title":"Memory Allocation","text":"<p>When any function is called from main(), the memory is allocated to it on the stack. A recursive function calls itself, the memory for a called function is allocated on top of memory allocated to calling function and different copy of local variables is created for each function call. When the base case is reached, the function returns its value to the function by whom it is called and memory is de-allocated and the process continues.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/#tailed-recursion","title":"Tailed recursion","text":"<p>A recursive function is tail recursive when recursive call is the last thing executed by the function.The tail recursive functions considered better than non tail recursive functions as tail-recursion can be optimized by the compiler. Compilers usually execute recursive procedures by using a stack. This stack consists of all the pertinent information, including the parameter values, for each recursive call. When a procedure is called, its information is pushed onto a stack, and when the function terminates the information is popped out of the stack. Thus for the non-tail-recursive functions, the stack depth (maximum amount of stack space used at any time during compilation) is more. The idea used by compilers to optimize tail-recursive functions is simple, since the recursive call is the last statement, there is nothing left to do in the current function, so saving the current function\u2019s stack frame is of no use.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/#why-recursion-works","title":"Why Recursion Works","text":"<p>In a recursive algorithm, the computer \"remembers\" every previous state of the problem. This information is \"held\" by the computer on the \"activation stack\" (i.e., inside of each functions workspace).</p> <p>Every function has its own workspace PER CALL of the function.</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/#general-observationtechniques","title":"General Observation/Techniques","text":"<ul> <li>Try to visualise the problem in terms of recursion tree representation of the problem</li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/#-try-guessing-the-time-and-space-complexity","title":"-   Try guessing the time and space Complexity","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Recursion/#loops-and-tail-recursion","title":"Loops and Tail Recursion","text":"<p>Some recursive algorithms are very similar to loops. These algorithms are called \"tail recursive\" because the last statement in the algorithm is to \"restart\" the algorithm. Tail recursive algorithms can be directly translated into loops.</p> <p>Questions * Tower of Hanoo * Inorder/PreOrder/PostOrder traversal of a tree.</p> <ul> <li>Subset<ul> <li>Generate Subset</li> <li>Subset Sum</li> </ul> </li> <li>Combination Sum Good Question</li> <li>All Permutations</li> <li>Print all possible expressions that evaluate to a target(Similar)Todo GFG</li> <li>Generate all binary strings without consecutive 1\u2019s Todo</li> <li>Recursive solution to count substrings with same first and last characters Todo</li> <li>Combinations in a String of Digits Todo</li> <li>Permutation Sequence</li> <li>Palindrome partitioning , GFG Using recursion</li> <li>Median of 2 sorted Array<ul> <li>Same Size</li> <li>Different Size<ul> <li>Linear(Merge both array), Time Complexity <code>O(m+n)</code>, Auxiliary Space <code>O(1)</code></li> <li>Using Binary Search, Time Complexity <code>O(min(log m, log n))</code> ,Auxiliary Space <code>O(1)</code></li> <li>Problems<ul> <li>Leetcode - <code>O(log (m+n))</code></li> </ul> </li> </ul> </li> </ul> </li> <li>Matrix Multiplication<ul> <li></li> </ul> </li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Segment_tree/","title":"Segment tree","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Segment_tree/#problems","title":"Problems","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Segment_tree/#-count-of-range-sum","title":"- Count of Range Sum","text":""},{"location":"Notes/Data_Structure_And_Algorithm/Stack_And_Queues/","title":"Stack And Queues","text":"<p>Stack is an abstract data type that serves as a collection of elements, with two main principal operations:</p> <ul> <li>Push, which adds an element to the collection, and</li> <li>Pop, which removes the most recently added element that was not yet removed.</li> </ul> <p></p> <p>The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out).Additionally, a peek operation may give access to the top without modifying the stack</p> <p>Monotonic stack is a stack whose elements are monotonically increasing or descreasing.If we need to pop smaller elements from the stack before pushing a new element, the stack is decreasing from bottom to top.Otherwise, it's increasing from bottom to top. For example,</p> <pre><code>Mono-decreasing Stack\nBefore: [5,4,2,1]\nTo push 3, we need to pop smaller (or equal) elements first\nAfter: [5,4,3]\n</code></pre> <p>Queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and the removal of entities from the other end of the sequence.The operations of a queue make it a first-in-first-out (FIFO) data structure.Queue has  two main principal operations:</p> <ul> <li>enque, add an element to the back</li> <li>deque, removes an element from the front</li> </ul> <p></p>"},{"location":"Notes/Data_Structure_And_Algorithm/Stack_And_Queues/#problems","title":"Problems","text":"<ul> <li>Queue using stack</li> <li>Stack using two queues</li> <li>Next Greater Element - Linear TC</li> <li>Cache Implementation using stacks and queue<ul> <li>LRU Cache</li> <li>LFU Cache</li> </ul> </li> <li></li> </ul>"},{"location":"Notes/Data_Structure_And_Algorithm/Strategy/","title":"Strategy","text":"<ul> <li>Suggestive pattern for problem solution classification <ul> <li>https://algo.monster/flowchart</li> </ul> </li> <li>Roadmap<ul> <li>https://neetcode.io/roadmap</li> </ul> </li> </ul> <p>Links with mentioned patterns for DSA questions * https://algo.monster/problems/two_sum_sorted * https://www.techinterviewhandbook.org/algorithms/array/ * https://github.com/Chanda-Abdul/Several-Coding-Patterns-for-Solving-Data-Structures-and-Algorithms-Problems-during-Interviews **  * https://www.educative.io/courses/grokking-coding-interview-patterns-java ** Grokking * https://www.designgurus.io/blog/Grokking-the-Coding%20Interview-Patterns * https://levelup.gitconnected.com/dont-just-leetcode-follow-the-coding-patterns-instead-4beb6a197fdb * https://liuzhenglaichn.gitbook.io/</p> <p>List of pattern * Binary Search * Array     * Two Pointers         * Same direction         * Opposite direction         * Sliding Window         * Prefix Sum         * Cycle finding         * Fast and slow moving pointer * Matrix * Queue * Interval     * LC Questions         * https://leetcode.com/problems/summary-ranges/         * https://leetcode.com/problems/merge-intervals/         * https://leetcode.com/problems/insert-interval         * https://leetcode.com/problems/minimum-number-of-arrows-to-burst-balloons/         * https://leetcode.com/problems/meeting-rooms/         * https://leetcode.com/problems/meeting-rooms-ii/ * Stack     * Basic Stack         * https://leetcode.com/problems/min-stack/         * https://leetcode.com/problems/evaluate-reverse-polish-notation/description/     * Monotonic Stack         * https://leetcode.com/problems/daily-temperatures/description/         * https://leetcode.com/problems/car-fleet/description/  (Todo)         * https://leetcode.com/problems/largest-rectangle-in-histogram/description/ (Todo) * Linked List     * https://leetcode.com/problems/linked-list-cycle * Binary Tree     * General         * https://leetcode.com/problems/maximum-depth-of-binary-tree         * https://leetcode.com/problems/same-tree/         * https://leetcode.com/problems/invert-binary-tree         * https://leetcode.com/problems/symmetric-tree         * https://leetcode.com/problems/binary-tree-level-order-traversal             * https://leetcode.com/problems/binary-tree-right-side-view         * https://leetcode.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/         * https://leetcode.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/         * https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/     * Breadth first search     * Depth first search * Binary Search Tree     * https://leetcode.com/problems/minimum-absolute-difference-in-bst/     * https://leetcode.com/problems/kth-smallest-element-in-a-bst/     * https://leetcode.com/problems/validate-binary-search-tree/ * Priority Queue/Heap Heap_Priority_Queue     * Top K     * Moving best     * Multiple heap     * Two Heap     * Misc         * https://leetcode.com/problems/meeting-rooms-ii * Graph     * Misc         * https://leetcode.com/problems/number-of-islands/description/ (             * Not exactly a Graph represented problem, but can be solved using concepts of BFS &amp; DFS, First part is to visualise that it can be solved via BFS/DFS then try to implement the same.         * https://leetcode.com/problems/surrounded-regions/             * Think about it from every angle     * Union Find | Disjoint set         *      * Breath first search     * Depth first search     * Path search         * https://leetcode.com/problems/evaluate-division/     * Directed Graph / Topological sort     * Weighted Graph     * Minimum spanning tree * Backtracking     * Combinatorial Search     * Pruning     * Aggregation and Memoization     * Dedup * Dynamic Programming     * Grid     * Dynamic number of subproblems     * Interval     * Dual-Sequence     * KnapSack     * Bitmask * Sorting     * Cyclic sort * Trie * Segment Tree * Bit Manipulation</p> <ul> <li>Extra topics : Not Important <ul> <li>Interval tree</li> <li></li> </ul> </li> </ul> <p>General Implementation pattern : https://algo.monster/templates</p> <p>Curate the Lists from following links * https://www.youtube.com/@EricProgramming * https://takeuforward.org/playlists/top-array-interview-questions-structured-path-with-video-solutions/ * https://leetcode.com/studyplan/top-interview-150/ * https://takeuforward.org/strivers-a2z-dsa-course/strivers-a2z-dsa-course-sheet-2/</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Time%20Complexity/","title":"Time Complexity","text":"<p>Best Case   : Omega  Avg Case    : Theta Worst case :  O</p>"},{"location":"Notes/Data_Structure_And_Algorithm/Union_Find%28Disjoint_Set%29/","title":"Union Find(Disjoint Set)","text":"<p>![[Pasted image 20240518105930.png]] Terminologies - Parent node: the direct parent node of a vertex. For example, in Figure 5, the parent node of vertex 3 is 1, the parent node of vertex 2 is 0, and the parent node of vertex 9 is 9. - Root node: a node without a parent node; it can be viewed as the parent node of itself. For example, in Figure 5, the root node of vertices 3 and 2 is 0. As for 0, it is its own root node and parent node. Likewise, the root node and parent node of vertex 9 is 9 itself. Sometimes the root node is referred to as the head node.</p> <p>General Steps - The\u00a0<code>find</code>\u00a0function\u00a0finds the root node of a given vertex - The\u00a0<code>union</code>\u00a0function\u00a0unions two vertices and makes their root nodes the same</p> <p>There are two ways to implement a \u201cdisjoint set\u201d. - Implementation with Quick Find:      - Data Structure :  An Array with index representing the vertex and the the value for a given index is the Root Vertex of it     - Time Complexity         - Find : O(n)         - Union : O(n^2) - Implementation with Quick Union:      - Data Structure : An Array with index representing the vertex and the the value for a given index is the Root or parent Vertex of it     - compared with the Quick Find implementation, the time complexity of the\u00a0<code>union</code>\u00a0function is better. Meanwhile, the\u00a0<code>find</code>\u00a0function will take more time in this case.</p>"},{"location":"Notes/Design%20Pattern/Strategy%20Design%20pattern/","title":"Strategy Design pattern","text":"<ul> <li>How to approach the problem<ul> <li>Rough flow</li> <li>Requirement gathering / clarification</li> <li></li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes/1.%20K8s%201O1/","title":"1. K8s 1O1","text":"<p>Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn't matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.</p> <p>K8s Components</p> <p> * Control Plane     * etcd : Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.(make sure you have a back up plan for those data.)     * kube-Api server : It exposes the Kubernetes API, designed to scale horizontally.      * kube-scheduler : Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.     * kube-control-manager : Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.     * cloud-controller-manager : The cloud-controller-manager only runs controllers that are specific to your cloud provider.         * The following controllers can have cloud provider dependencies:             * Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding             * Route controller: For setting up routes in the underlying cloud infrastructure             * Service controller: For creating, updating and deleting cloud provider load balancers * Worker Nodes     *  kubelet : An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod defined in PodSpecs.     *  kube-proxy : Network proxy that runs on each node in your cluster.     * Container runtime :  The container runtime is the software that is responsible for running containers. *  Addons     * DNS : Containers started by Kubernetes automatically include this DNS server in their DNS searches.</p>"},{"location":"Notes/Kubernetes/2.%20Objects%20and%20metadata/","title":"2. Objects and metadata","text":"<p>Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe: -   What containerized applications are running (and on which nodes) -   The resources available to those applications -   The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance</p> <p>Object spec and status * Spec : The desired state of an object * Status : The actual state of an object Describing a Kubernetes object through API/yaml -   <code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object -   <code>kind</code> - What kind of object you want to create -   <code>metadata</code> - Data that helps uniquely identify the object, including a <code>name</code> string, <code>UID</code>, and optional <code>namespace</code> -   <code>spec</code> - What state you desire for the object Management Techniques of objects via API/kubectl *  Imperative commands : Live objects - Development projects     * eg  : kubectl create deployment nginx --image nginx * Imperative object configuration : Individual files - Production projects     * eg : kubectl delete -f nginx.yaml -f redis.yaml * Declarative object configuration : Directories of files - Production projects     * User does not define the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by <code>kubectl</code>     * eg :  <pre><code>kubectl diff -f configs/\nkubectl apply -f configs/\n</code></pre></p> <ul> <li>Metadata<ul> <li>Object Names and IDs <ul> <li>Each object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID (Kubernetes systems-generated) that is unique across your whole cluster.</li> </ul> </li> <li>Labels and Selectors<ul> <li>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time.</li> <li>Selectors are used by the client/user can identify a set of objects.<ul> <li>equality-based : <ul> <li>environment = production</li> <li>tier != frontend</li> <li>environment=production,tier!=frontend</li> </ul> </li> <li>set-based  : <ul> <li>environment in (production, qa)</li> <li>tier notin (frontend, backend)</li> <li>partition</li> <li>!partition</li> </ul> </li> <li>List and Watching using kubectl/API<ul> <li>kubectl get pods -l 'environment in (production),tier in (frontend)'</li> </ul> </li> </ul> </li> </ul> </li> <li>Namespaces <ul> <li>Mechanism for isolating groups of resources within a single cluster.Namespaces are a way to divide cluster resources between multiple users (via resource quota).</li> <li>Namespaces and DNS : When you create a Service, it creates a corresponding DNS entry. This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code></li> </ul> </li> <li>Annotations<ul> <li>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.</li> <li>Example<ul> <li>Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto-generated fields and fields set by auto-sizing or auto-scaling systems.</li> <li>Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address.</li> <li></li> </ul> </li> </ul> </li> <li>Field Selector<ul> <li>Field selectors let you select Kubernetes resources based on the value of one or more resource fields.</li> </ul> </li> <li>Finalizers<ul> <li>Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.</li> <li>Example<ul> <li>A common example of a finalizer is <code>kubernetes.io/pv-protection</code>, which prevents accidental deletion of <code>PersistentVolume</code> objects. When a <code>PersistentVolume</code> object is in use by a Pod, Kubernetes adds the <code>pv-protection</code> finalizer. If you try to delete the <code>PersistentVolume</code>, it enters a <code>Terminating</code> status, but the controller can't delete it because the finalizer exists. When the Pod stops using the <code>PersistentVolume</code>, Kubernetes clears the <code>pv-protection</code> finalizer, and the controller deletes the volume</li> </ul> </li> </ul> </li> <li>Owners and Dependents<ul> <li>In Kubernetes, some objects are owners of other objects.Dependent objects also have an <code>ownerReferences.blockOwnerDeletion</code> field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object. Kubernetes automatically sets this field to <code>true</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes/3.%20Architecture/","title":"3. Architecture","text":""},{"location":"Notes/Kubernetes/3.%20Architecture/#nodes","title":"Nodes","text":"<ul> <li>A node may be a virtual or physical machine, depending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods.The components on a node include the kubelet, a container runtime, and the kube-proxy.</li> <li>Management<ul> <li>There are two main ways to have Nodes added to the API server:<ul> <li>The kubelet on a node self-registers to the control plane</li> <li>You (or another human user) manually add a Node object</li> </ul> </li> </ul> </li> <li>Node Status<ul> <li>Addresses <ul> <li>HostName : The hostname as reported by the node's kernel. Can be overridden via the kubelet <code>--hostname-override</code> parameter.</li> <li>ExternalIP : Typically the IP address of the node that is externally routable (available from outside the cluster).</li> <li>InternalIP : InternalIP: Typically the IP address of the node that is routable only within the cluster.</li> </ul> </li> <li>Conditions : The <code>conditions</code> field describes the status of all <code>Running</code> nodes.</li> <li>Capacity and Allocatable : The fields in the capacity block indicate the total amount of resources that a Node has. The allocatable block indicates the amount of resources on a Node that is available to be consumed by normal Pods.</li> <li>Info : </li> </ul> </li> <li>Heartbeats<ul> <li>Sent by Kubernetes nodes, help your cluster determine the availability of each node, and to take action when failures are detected<ul> <li>updates to the <code>.status</code> of a Node</li> <li>Lease objects within the <code>kube-node-lease</code> namespace. Each Node has an associated Lease object.Distributed systems often have a need for \"leases\", which provides a mechanism to lock shared resources and coordinate activity between nodes.In Kubernetes, the \"lease\" concept is represented by <code>Lease</code> objects in the <code>coordination.k8s.io</code> API group, which are used for system-critical capabilities like node heart beats and component-level leader election.</li> </ul> </li> </ul> </li> <li>Node controller <ul> <li>Is a Kubernetes control plane component that manages various aspects of nodes.</li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes/3.%20Architecture/#control-plane","title":"Control Plane","text":""},{"location":"Notes/Kubernetes/3.%20Architecture/#controllers","title":"Controllers","text":"<ul> <li>A control loop is a non-terminating loop that regulates the state of a system.In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.</li> <li>Controller pattern<ul> <li>Control via API server : </li> <li>Direct control : Controllers that interact with external state find their desired state from the API server, then communicate directly with an external system to bring the current state closer in line.</li> </ul> </li> <li>Design<ul> <li>As a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect of cluster state. Most commonly, a particular control loop (controller) uses one kind of resource as its desired state, and has a different kind of resource that it manages to make that desired state happen. For example, a controller for Jobs tracks Job objects (to discover new work) and Pod objects (to run the Jobs, and then to see when the work is finished). In this case something else creates the Jobs, whereas the Job controller creates Pods.</li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes/3.%20Architecture/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<ul> <li>The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.</li> <li>Responsibilities<ul> <li>Node Controller : The node controller is responsible for updating Node objects when new servers are created in your cloud infrastructure.</li> <li>Route Controller : The route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in your Kubernetes cluster can communicate with each other.</li> <li>Service Controller : The service controller interacts with your cloud provider's APIs to set up load balancers and other infrastructure components when you declare a Service resource that requires them.</li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes/5.%20Workloads/","title":"5. Workloads","text":"<p>A workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on Kubernetes you run it inside a set of pods. In Kubernetes, a <code>Pod</code> represents a set of running containers on your cluster.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#pod","title":"Pod","text":"<p>A Pod (smallest deployable units of computing that you can create and manage in Kubernetes.) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context.</p> <p>The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.Each Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (to provide more overall resources by running more instances), you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#grouping-multiple-co-located-and-co-managed-containers-in-a-single-pod-is-a-relatively-advanced-use-case-you-should-use-this-pattern-only-in-specific-instances-in-which-your-containers-are-tightly-coupled","title":"Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled.","text":"<p>Pods natively provide two kinds of shared resources for their constituent containers: networking and storage Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them,Whereas most Pods are managed by the control plane.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#pod-lifecycle","title":"Pod Lifecycle","text":"<p>Phases of a Pod * <code>Pending</code> : The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network. * <code>Running</code> : The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting. * <code>Succeded</code> : All containers in the Pod have terminated in success, and will not be restarted. * <code>Failed</code> : All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system. * <code>Unknown</code> : For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running. Container States : As well as the phase of the Pod overall, Kubernetes tracks the state of each container inside a Pod. You can use container lifecycle hooks to trigger events to run at certain points in a container's lifecycle. * <code>Waiting</code> : A container in the <code>Waiting</code> state is still running the operations it requires in order to complete start up: for example, pulling the container image from a container image registry, or applying Secret data. * <code>Running</code> : The <code>Running</code> status indicates that a container is executing without issues. * <code>Terminated</code> : A container in the Terminated state began execution and then either ran to completion or failed for some reason Container restart policy : The <code>spec</code> of a Pod has a <code>restartPolicy</code> field with possible values Always, OnFailure, and Never. The default value is Always.The <code>restartPolicy</code> applies to all containers in the Pod. Pod conditions A Pod has a PodStatus, which has an array of PodConditions through which the Pod has or has not passed. Kubelet manages the following PodConditions: -   <code>PodScheduled</code>: the Pod has been scheduled to a node. -   <code>PodHasNetwork</code>: (alpha feature; must be enabled explicitly) the Pod sandbox has been successfully created and networking configured. -   <code>ContainersReady</code>: all containers in the Pod are ready. -   <code>Initialized</code>: all init containers have completed successfully. -   <code>Ready</code>: the Pod is able to serve requests and should be added to the load balancing pools of all matching Services Container Probes : A probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet either executes code within the container, or makes a network request * Check Mechanism      * <code>exec</code> : Executes a specified command inside the container. The diagnostic is considered successful if the command exits with a status code of 0.     * <code>grpc</code> : The diagnostic is considered successful if the <code>status</code> of the response is <code>SERVING</code>.     * <code>httpGet</code> : The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.     * <code>tcpSocket</code> :  Performs a TCP check against the Pod's IP address on a specified port. The diagnostic is considered successful if the port is open. If the remote system (the container) closes the connection immediately after it opens, this counts as healthy. * Probe Outcome     * Success     * Failure     * Unknown * Types of probe     * <code>livenessProbe</code>     * <code>readinessProbe</code>     * <code>startupProbe</code></p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#init-containers","title":"Init Containers","text":"<p>Specialized containers that run before app containers in a Pod.If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. However, if the Pod has a <code>restartPolicy</code> of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.To specify an init container for a Pod, add the <code>initContainers</code> field into the Pod specification, as an array of <code>container</code> items.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#ephemral-container","title":"Ephemral Container","text":"<p>Sometimes it's necessary to inspect the state of an existing Pod, however, for example to troubleshoot a hard-to-reproduce bug. In these cases you can run an ephemeral container in an existing Pod to inspect its state and run arbitrary commands.Ephemeral containers are useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient because a container has crashed or a container image doesn't include debugging utilities.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<p>Kubernetes assigns every Pod a QoS class based on the resource requests and limits of its component Containers.QoS classes are used by Kubernetes to decide which Pods to evict from a Node experiencing Node Pressure. * Classes     * Guaranteed : Pods that are <code>Guaranteed</code> have the strictest resource limits and are least likely to face eviction.Every Container in the Pod must have a memory/CPU limit which must be equal to memory /CPUrequest.     * Burstable : The Pod does not meet the criteria for QoS class <code>Guaranteed</code>. At least one Container in the Pod has a memory or CPU request or limit     * BestEffort : A Pod has a QoS class of <code>BestEffort</code> if it doesn't meet the criteria for either <code>Guaranteed</code> or <code>Burstable</code>. In other words, a Pod is <code>BestEffort</code> only if none of the Containers in the Pod have a memory limit or a memory request, and none of the Containers in the Pod have a CPU limit or a CPU request.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#workload-resources","title":"WorkLoad Resources","text":""},{"location":"Notes/Kubernetes/5.%20Workloads/#deployments","title":"Deployments","text":"<p>A Deployment provides declarative updates for Pods and ReplicaSets.You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#use-case","title":"Use Case","text":"<p>The following are typical use cases for Deployments: -   Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not. -   Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment. -   Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment. -   Scale up the Deployment to facilitate more load. -   Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout. -   Use the status of the Deployment as an indicator that a rollout has stuck. -   Clean up older ReplicaSets that you don't need anymore</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#replicaset","title":"ReplicaSet","text":"<p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all.</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#statefulset","title":"StatefulSet","text":"<p>Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling StatefulSets are valuable for applications that require one or more of the following. -   Stable, unique network identifiers. -   Stable, persistent storage. -   Ordered, graceful deployment and scaling. -   Ordered, automated rolling updates. Limitations -   The storage for a given Pod must either be provisioned by a PersistentVolume Provisioner based on the requested <code>storage class</code>, or pre-provisioned by an admin. -   Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources. -   StatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. You are responsible for creating this Service. -   StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion. -   When using Rolling Updates with the default Pod Management Policy (<code>OrderedReady</code>), it's possible to get into a broken state that requires manual intervention to repair</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#daemonset","title":"DaemonSet","text":"<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.Some typical uses of a DaemonSet are: -   running a cluster storage daemon on every node -   running a logs collection daemon on every node -   running a node monitoring daemon on every node</p>"},{"location":"Notes/Kubernetes/5.%20Workloads/#job","title":"Job","text":"<p>A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will delete its active Pods until the Job is resumed again.</p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/","title":"6. Services, Load Balancing and Networking","text":"<p>Kubernetes imposes the following fundamental requirements on any networking implementation (barring any intentional network segmentation policies): - pods can communicate with all other pods on any other node without NAT. This is called the \"IP-per-pod\" model. - agents on a node (e.g. system daemons, kubelet) can communicate with all pods on that node Kubernetes networking addresses four concerns: - Containers within a Pod use networking to communicate via loopback. - Cluster networking provides communication between different Pods. - The Service API lets you expose an application running in Pods to be reachable from outside your cluster.     - Ingress provides extra functionality specifically for exposing HTTP applications, websites and APIs. - You can also use Services to publish services only for consumption inside your cluster.</p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#service","title":"Service","text":"<p>In Kubernetes , a service is a method for exposing a networking application that is running as one or more Pods in Your cluster, is an abstraction to help you expose groups of Pods over a network. The set of Pods targeted by a Service is usually determined by a selector that you define. </p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#defining-a-service","title":"Defining a service","text":"<p>For example, suppose you have a set of Pods that each listen on TCP port 80 and are labelled as <code>app.kubernetes.io/name=proxy</code>. You can define a Service to publish that TCP listener, Port definitions in Pods have names, and you can reference these names in the <code>targetPort</code> attribute of a Service. <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app.kubernetes.io/name: proxy\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    ports:\n      - containerPort: 80\n        name: http-web-svc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app.kubernetes.io/name: proxy\n  ports:\n  - name: name-of-service-port\n    protocol: TCP\n    port: 80\n    targetPort: http-web-svc\n</code></pre></p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#services-without-selectors","title":"Services without selectors","text":"<p>Services most commonly abstract access to Kubernetes Pods thanks to the selector, but when used with a corresponding set of EndpointSlices objects and without a selector, the Service can abstract other kinds of backends, including ones that run outside the cluster.</p> <p>For example: - You want to have an external database cluster in production, but in your test environment you use your own databases. - You want to point your Service to a Service in a different Namespace or on another cluster. - You are migrating a workload to Kubernetes. While evaluating the approach, you run only a portion of your backends in Kubernetes.</p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#multi-port-services","title":"Multi-port Services","text":"<p>For some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service object. When using multiple ports for a Service, you must give all of your ports names so that these are unambiguous. For example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 9376\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 9377\n</code></pre>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#service-type","title":"Service type","text":"<p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, one that's accessible from outside of your cluster. Kubernetes Service types allow you to specify what kind of Service you want.</p> <p>The available <code>type</code> values and their behaviors are: <code>ClusterIP</code> Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default that is used if you don't explicitly specify a <code>type</code> for a Service. You can expose the Service to the public internet using an Ingress or a Gateway. <code>NodePort</code> Exposes the Service on each Node's IP at a static port (the <code>NodePort</code>). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service of <code>type: ClusterIP</code>. <code>LoadBalancer</code> Exposes the Service externally using an external load balancer. Kubernetes does not directly offer a load balancing component; you must provide one, or you can integrate your Kubernetes cluster with a cloud provider. <code>ExternalName</code> Maps the Service to the contents of the <code>externalName</code> field (for example, to the hostname <code>api.foo.bar.example</code>). The mapping configures your cluster's DNS server to return a <code>CNAME</code> record with that external hostname value. No proxying of any kind is set up.</p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#ingress","title":"Ingress","text":"<p>An API object that manages external access to the services in a cluster, typically HTTP. Ingress may provide load balancing, SSL termination and name-based virtual hosting.</p>"},{"location":"Notes/Kubernetes/6.%20Services%2C%20Load%20Balancing%20and%20Networking/#prerequisite","title":"Prerequisite","text":"<p>You must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect. You may need to deploy an Ingress controller such as ingress-nginx. You can choose from a number of Ingress controllers. A minimal Ingress resource example: <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: minimal-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx-example\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        pathType: Prefix\n        backend:\n          service:\n            name: test\n            port:\n              number: 80\n</code></pre></p>"},{"location":"Notes/Python/Frequently_Used_Patterns/","title":"Frequently Used Patterns","text":"<ul> <li>General List</li> <li>Sorting a list of tuple on a single element     <code>python      tup = [(1,2),(3,4)]      tup.sort(key = lambda x: x[1]) # Sort it by second element</code></li> <li>Sort list of tuples by specific ordering</li> <li>Sort list of tuple with multiple keys with precedence, E.g. : Here the tuple list is sorted by following element order : second element , first element      <code>python      sorted_lst_of_tuples = sorted(lst_of_tuples, key=lambda x: (x[1], x[0]))</code></li> <li>Initialise an array with known size      <code>python     arr = [0]*n</code> </li> <li>Single liner list transformation <pre><code>lp = [1,2,3]\nop = [ str(x) for x in lp ] \n# op = ['1','2','3']\n</code></pre></li> <li>Join List of string <pre><code>text = ['Python', 'is', 'a', 'fun', 'programming', 'language']\n# join elements of text with space\nprint(' '.join(text))\n# Output: Python is a fun programming language\n</code></pre></li> </ul>"},{"location":"Notes/System_Design/Elements/","title":"Elements","text":"<ul> <li> <p>Database schema crawler</p> <ul> <li>https://www.schemacrawler.com/</li> <li>Almost all database with JDBC connectivity</li> <li></li> </ul> </li> <li> <p>DNS : For Domain name resolution</p> </li> <li>Load Balancer</li> <li>Database replication for read and write seperation</li> <li>Cache : To stop DB reads for same data</li> <li>CDN for static network</li> <li>Message Queue</li> <li>Logging / metrics</li> </ul>"},{"location":"Notes/System_Design/Elements/#relationl-db","title":"Relationl DB","text":"<ul> <li>Mysql / Oracle database / PostgreSQL</li> </ul>"},{"location":"Notes/System_Design/Elements/#non-relational-db","title":"Non-Relational DB","text":"<ul> <li>Neo4j / Cassandra / HBase / AWS DynamoDB</li> <li>Categories<ul> <li>Key-Value Stores</li> <li>Graph Stores</li> <li>Column Stores</li> <li>Document Stores</li> </ul> </li> <li>Why to choose<ul> <li>Super low latency</li> <li>UnStructured Data</li> <li>Store Massive amount of data</li> </ul> </li> </ul>"},{"location":"Notes/System_Design/Elements/#scaling","title":"Scaling","text":"<ul> <li>Vertical Scaling - Scale up</li> <li>Horizontal scaling - Scale out - Sharding</li> </ul>"},{"location":"Notes/System_Design/Elements/#steps","title":"Steps","text":"<ul> <li>Understand the problem and establish design scope</li> <li>Propose high-level design and get buy-in</li> <li>Design deep dive</li> <li></li> </ul>"},{"location":"Notes/System_Design/Elements/#consistent-hashing","title":"Consistent Hashing","text":"<p>Consistent hashing is a special kind of hashing such that when a hash table is re-sized and consistent hashing is used, only k/n keys need to be remapped on average, where k is the number of keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped.</p>"},{"location":"blog/","title":"Blog","text":""}]}